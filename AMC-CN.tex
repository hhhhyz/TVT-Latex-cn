%!TEX program=xelatex

% 碰到Windows版本提示Fandol字体，可以在命令行中以管理员权限执行：tlmgr update -self -all
%\documentclass[review]{cvpr}
\documentclass[final]{cvpr}

\usepackage[UTF8]{ctex}

%\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{overpic}
\usepackage{enumitem}
\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}


\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{159} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2020}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\cmm}[1]{\textcolor[rgb]{0,0.6,0}{CMM: #1}}
\newcommand{\todo}[1]{{\textcolor{red}{\bf [#1]}}}
\newcommand{\alert}[1]{\textcolor[rgb]{.6,0,0}{#1}}

\newcommand{\IT}{IT\cite{98pami/Itti}}
\newcommand{\MZ}{MZ\cite{03ACMMM/Ma_Contrast-based}}
\newcommand{\GB}{GB\cite{conf/nips/HarelKP06}}
\newcommand{\SR}{SR\cite{07cvpr/hou_SpectralResidual}}
\newcommand{\FT}{FT\cite{09cvpr/Achanta_FTSaliency}}
\newcommand{\CA}{CA\cite{10cvpr/goferman_context}}
\newcommand{\LC}{LC\cite{06acmmm/ZhaiS_spatiotemporal}}
\newcommand{\AC}{AC\cite{08cvs/achanta_salient}}
\newcommand{\HC}{HC-maps }
\newcommand{\RC}{RC-maps }
\newcommand{\Lab}{$L^*a^*b^*$}
\newcommand{\mypara}[1]{\paragraph{#1.}}

\graphicspath{{figures/}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}

\begin{document}
% \begin{CJK*}{GBK}{song}

\renewcommand{\figref}[1]{图\ref{#1}}
\renewcommand{\tabref}[1]{表\ref{#1}}
\renewcommand{\equref}[1]{式\ref{#1}}
\renewcommand{\secref}[1]{第\ref{#1}节}
\def\abstract{\centerline{\large\bf 摘要} \vspace*{12pt} \it}

%%%%%%%%% TITLE

\title{自动调制分类：一种支持深度学习的方法\thanks{本文为IEEE TVT发表的对应文献的中文翻译版。}}

\author{Fan Meng$^{1}$, Peng Chen$^{1}$, Member, IEEE, Lenan Wu$^{1}$, and Xiaobin Wang$^{2}$, Fellow, IEEE \\
    $^{1}$ Southeast University \quad \quad
    $^2$ Western University   \quad \quad \\
}

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
自动调制分类(AMC)在民用和军事应用中都起着至关重要的作用，本文通过深度学习的方法对其进行了研究。传统的AMC可以分为基于最大似然(ML)的AMC(ML-AMC)和基于特征的AMC。然而，ML-AMCS的计算复杂度较高，其实际应用难度较大，人工提取的特征需要专家知识。为此，本文提出了一种基于端到端卷积神经网络(CNN)的自适应AMC(CNN-AMC)方法，该方法自动从长符号率观测序列中提取特征和估计的信噪比(SNR)。对于CNN-AMC，采用单元分类器来适应不同的输入维度。针对CNN-AMC直接训练模型复杂、任务复杂的问题，提出了一种新的两步训练方法，并引入了迁移学习来提高再训练的效率。在不同的场景下考虑了不同的数字调制方案，仿真结果表明CNN-AMC的性能优于基于特征的方法，并且更接近最优ML-AMC。此外，CNN-AMC对载波相位偏差和信噪比估计误差具有一定的鲁棒性。在并行计算的情况下，基于深度学习的方法在推理速度方面比ML-AMC快40到1700倍。\\ 
{\bf\emph{ 关键词——\ 自动调制分类；卷积神经网络；深度学习；两步训练 }\rm}
\end{abstract}





%%%%%%%%% BODY TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{引言}\label{sec:Introduction}
为了实现最大可能的传输速率，自适应调制方案被采用在民用无线通信系统中以达到充分利用时变信号。在这种情况下，有关通信过程中使用的特定调制方案的信息必须由发射机通过网络协议与接收机共享，以协议开销为代价。当接收器具有调制方案识别能力时，可以消除这种开销。另一方面，许多军事应用也需要自动发现敌方信号使用的调制方案。这些应用包括信号拦截和干扰。因此，自动调制分类(AMC)~\cite{r1}~\cite{r2}对于实现自动接收机配置、干扰抑制和频谱管理的民用和军用应用都具有重要意义。

传统的基于特征的AMC算法~\cite{r3}、~\cite{r4}可以通过数据预处理、特征提取和分类决策三个步骤来实现。由于其良好的统计特征能够以较低的复杂度提供稳健的性能~\cite{r5}，基于特征提取的AMC方法已经在许多已有的文献~\cite{r6}、~\cite{r7}中得到了研究，如循环特征~\cite{r8}、高阶矩和累积量~\cite{r9}-~\cite{r11}、小波变换~\cite{r12}、~\cite{r13}等。在分类决策中，传统的分类器包括随机森林~\cite{r14}、多层感知器~\cite{r15}、~\cite{r16}和支持向量机(SVM)~\cite{r17}、~\cite{r18}。这些AMC是不同特征提取算法和分类的组合，需要专家知识和人工设计。

文献~\cite{r19}也研究了基于最大似然(ML)的AMC方法，其中ML-AMC已经被证明在使用数学信道模型的场景中达到了最优的性能。与基于特征的AMC不同，ML-AMC计算所有候选调制的似然函数，并选择似然值最大的调制方案，作为端到端模块工作。
然而，较高的计算复杂度使得低成本的实时实现变得极其困难~\cite{r20}。此外，在复杂的环境下，ML-AMC的精确概率是很难实现的。

在基于特征的AMC中，机器学习方法只是作为特征和多假设之间的映射函数。为了避免特征工程，深度学习方法~\cite{r21}近年来在算法设计和硬件实现方面发展迅速~\cite{r22}，它可以学习比浅层学习方法更复杂的函数。深度学习的快速发展已经在通信领域取得了一些成功的应用~\cite{r23}-~\cite{r25}，包括调制分类。在~\cite{r26}中，原始信号的眼图用作基于Lenet-5的分类器~\cite{r27}的输入，并巧妙地重新结合图像识别领域的研究，提出了AMC问题。文献~\cite{r28}、~\cite{r29}提出了一种用于无线电调制识别的卷积网络，并用RML2016.10b数据集进行了仿真，结果表明该方法的分类精度高于专家特征工程方法。然后在~\cite{r30}中，基于LSTM的AMC被证明在小尺度或中尺度上优于CNN模型的过采样接收信号。文献~\cite{r31}对长符号率信号进行了研究，堆叠式自动编码器在增加仿真次数的情况下取得了很好的效果。

本文提出了一种基于深度神经网络(DNN)的自适应AMC算法，它可以自动学习从低信噪比的长符号率信号中提取特征，并采用卷积神经网络（CNN）来实现自动调制分类。
CNN-AMC可以在性能损失最小的情况下接近ML-AMC，同时速度有显著提高。ML-AMC被用作我们提出的CNN-AMC的基准，并且还可以生成不同场景下的训练数据。这项工作的贡献概括如下：
\begin{itemize}
\item 直接利用低信噪比的长符号率信号，提出了一种基于深度神经网络的端到端可训练AMC算法。与基于特征的AMC不同，CNN-AMC自动学习从原始信号中提取特征，简化AMC设计。此外，CNN-AMC对ML-AMC进行了近似，相关处理可以并行完成，加快了推理速度。
\item  针对CNN-AMC的直接训练问题，我们提出了一种包括预训练和微调的两步训练方法。对于不同调制方式和通信信道场景的不同组合，还提出了迁移学习来进一步提高再训练效率。
\item  通过将经过长预处理的观测序列分成单元大小的信号段，引入单元输入维数以适应不同的输入维数，同时并行实现分块计算。 
\end{itemize}

本文的其余部分组织如下。第二节概述了经典决策理论框架下的调制分类问题。第三节给出了具有多输入特征的CNN-AMC的结构，以及相应的两步训练、迁移学习和单元分类器。然后，在第四节中，对所提出的CNN-AMC、ML-AMC和基于特征的方法在不同环境下的信噪比进行了测试，并对仿真结果进行了分析。
第五节给出了结论和讨论。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{系统模型}
\label{sec:SystemModel}
\subsection{问题表述}
\label{sec:Problemfor}
在不损失一般性的情况下，接收基带信号的复包络可以表示为:
\begin{equation}
y(t)=x\left(t ; \mathbf{u}_{k}\right)+v(t)\label{equation1}
\end{equation}
其中$v(t)$是具有零均值和双边功率谱密度为$N_0/2$的复加性高斯白噪声(AWGN)。无噪声信号$x\left(t ; \mathbf{u}_{k}\right)$通过综合形式被建模为:
\begin{equation}
x\left(t ; \mathbf{u}_{k}\right)=a e^{j\left(2 \pi \Delta f t+\theta_{c}\right)} \sum_{n=0}^{N-1} x_{n}^{k, i} g\left(t-n T_{s}-\varepsilon T_{s}\right)\label{equation2}
\end{equation}
其中，$\mathbf{u}_{k}$定义为多维参数集。其中，在第k次调制方案下，一串未知信号和信道变量是随机的或确定的，并且它被给出为:
\begin{equation}
\mathbf{u}_{k}=\left\{a, \theta_{c}, \Delta f,\left\{x^{k, i}\right\}_{i=1}^{M_{k}}, h(t), \varepsilon\right\}\label{equation3}
\end{equation}
在(\ref{equation2})和(\ref{equation3})中使用的符号如下所示：
\begin{itemize}
\item $a$，信号幅度。
\item $\theta_{c}$，因传播延迟及初始载波相位引入的固定相位偏移。
\item $N$，接收序列中的符号数，即观测序列空间。
\item $x^{k, i}$，第$k$调制方案下的第$i$星座点，其中符号均匀分布，
$i \in\left\{1, \ldots, M_{k}\right\}, k \in\{1, \ldots, C\}$，
标记$M_k$分别表示使用该调制的符号数目，$C$是候选等概率调制方案的数目。调制的码元通常用单位功率进行归一化。
\begin{equation}
\frac{1}{M_{k}} \sum_{i=1}^{M_{k}}\left|x^{k, i}\right|^{2}=1\label{equation4}
\end{equation}
此外，数据块中的符号被假定为独立同分布(\textbf{I.I.D})。
\item $\Delta f$，去除载波后的剩余载波频率。
\item $T_s$，符号间隔。
\item $g(t)$，剩余通道$h(t)$和脉冲整形函数$p(t)$的复合效应。它由方程$g(t)=h(t)∗p(t)$表示，其中$∗$是卷积算子。通常，$p(t)$是持续时间$t_s$的升余弦脉冲函数。
\item $\varepsilon$，发送器和接收器之间的时序偏移的归一化历元，$0≤ε<1$。
\end{itemize}

本文将接收信号表示为采样复数基带表示，并考虑了$\frac{E_s}{N_0}$的信噪比。以符号间隔$t_s$采样的离散观测序列可以写为$\mathbf{y}=[y_0，\dots，y_{N−1}]^T$，其中上标$T$是转置运算符。接收信号的每个采样时刻都是接收端脉冲整形匹配滤波器的输出，可以表示为:
\begin{equation}
y_{n}=\frac{1}{T_{s}} \int_{n T_{s}}^{(n+1) T_{s}} y(t) p\left(t-n T_{s}\right) d t \label{equation5}
\end{equation}
在盲分类的情况下，发送信号的脉冲形状$p(t)$对于接收器是未知的。同样，瞬时$y_n$通常归一化为单位功率。
\begin{equation}
y_{n}=\frac{y_{n}}{\sqrt{\frac{1}{N} \mathbf{y}^{H} \mathbf{y}}}\label{equation6}
\end{equation}
其中上标$H$是共轭转置运算符。在分类器中，需要估计符号信噪比，这相当于既知道$a$又知道$N_0$。

假设$H_k$下$y_n$的条件概率密度函数(\bf PDF\rm)可以表示为:
\begin{equation}
\begin{aligned}
&p\left(y_{n} \mid H_{k}, \mathbf{u}_{k}\right) =\sum_{i=1}^{M_{k}} \frac{1}{M_{k} \sqrt{\pi N_{0}}} \exp \left(\frac{\left|y_{n}-x_{n}^{k, i}\left(\mathbf{u}_{k}\right)\right|^{2}}{N_{0}}\right) \\
& \propto \frac{1}{M_{k}} \sum_{i=1}^{M_{k}} \exp \left(\frac{\left|x_{n}^{k, i}\right|^{2}-2 \Re \mathfrak{e}\left\{y_{n}^{*} x_{n}^{k, i}\left(\mathbf{u}_{k}\right)\right\}}{N_{0}}\right)
\end{aligned}\label{equation7}
\end{equation}

参数集$\mathbf{u_k}$中的元素是具有已知\textbf{PDFs}的确定性或随机变量。因此，可以通过取(\ref{equation7})的统计平均值来获得观测序列的边际密度函数，并且它被给出为:
\begin{equation}
\Gamma\left(y_{n} \mid H_{k}\right)=\mathbb{E}_{\mathbf{u}_{k}}\left\{\Gamma\left(y_{n} \mid H_{k}, \mathbf{u}_{k}\right)\right\}\label{equation8}
\end{equation}
其$\mathbb{E}\{·\}$是期望运算符。

假设信道环境变化缓慢，设置$\mathbf{u_k}$中的参数在整个观察期内是静态的。此外，归一化历元$\varepsilon=0$且噪声为白色，因此$y$中的元素服从独立同分布(\textbf{I.I.D}),函数$p(y_n|h_k，u_k)$表示时刻$n$处单次观测的概率密度，整个观测序列的联合似然函数可描述为:
\begin{equation}
\Gamma\left(\mathbf{y} \mid H_{k}\right)=\prod_{n=0}^{N-1} p\left(y_{n} \mid H_{k}\right)\label{equation9}
\end{equation}
数值计算中的浮点溢出可以由一个大的$N$引入，因此通常采用(\ref{equation9})的等效对数形式，它由下式给出:
\begin{equation}
\mathrm{L}\left(\mathbf{y} \mid H_{k}\right)=\ln \Gamma\left(\mathbf{y} \mid H_{k}\right)\label{equation10}
\end{equation}
根据最大似然准则，给定观测序列$\mathbf{y}$，最终选择与第$k$个调制方案对应的最可能的假设$H_k$，其似然值最大:
\begin{equation}
\widehat{k}=\arg \max _{k \in\{1, \ldots, C\}} \Gamma\left(\mathbf{y} \mid H_{k}\right)\label{equation11}
\end{equation}
其中，顶端标记($\widehat{.}$)表示估计值。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{正确概率}\label{sec:CorrectPro}

条件正确概率$P_{cc}(\mathbf{y}|H_K)$定义为:
\begin{equation}
P_{\mathrm{cc}}\left(\mathbf{y} \mid H_{k}\right)=\frac{\Gamma\left(\mathbf{y} \mid H_{k}\right)}{\sum_{k^{\prime}=1}^{C} \Gamma\left(\mathbf{y} \mid H_{k^{\prime}}\right)}\label{equation12}
\end{equation}
因此，总体正确概率$P_c$被表示为:
\begin{equation}
P_{\mathrm{c}}=\frac{1}{C} \sum_{k=1}^{C} P_{\mathrm{cc}}\left(\mathbf{y} \mid H_{k}\right)\label{equation13}
\end{equation}
观测序列$\mathbf{y}$的分布为$f_\mathbf{Y}(\mathbf{y})$，然后结合(\ref{equation12})和(\ref{equation13})，$P_c$的期望值由下式给出：
\begin{equation}
\begin{aligned}
\mathbb{E}_{\mathbf{Y}}\left\{P_{\mathrm{c}}\right\} &=\int_{\mathbf{y}} P_{\mathrm{c}} f_{\mathbf{Y}}(\mathbf{y}) d \mathbf{y} \\
&=\frac{1}{C} \sum_{k=1}^{C} \int_{\mathbf{y}} \frac{\Gamma\left(\mathbf{y} \mid H_{k}\right)}{\sum_{k^{\prime}=1}^{C} \Gamma\left(\mathbf{y} \mid H_{k^{\prime}}\right)} \Gamma\left(\mathbf{y} \mid H_{k}\right) d \mathbf{y} \label{equation14}
\end{aligned}
\end{equation}
假设参数集$\mathbf{u_k}$是已知的，但实际上，它通常是在建模或估计时会有一些偏差。然后将(\ref{equation14})表示为:
\begin{equation}
\mathbb{E}_{\mathbf{Y}}\left\{P_{\mathrm{c}}\right\}=\mathbb{E}_{\mathbf{U}_{k}, \hat{\mathbf{U}}_{k}, \mathbf{Y}}\left\{\frac{1}{C} \sum_{k=1}^{C} \int_{\mathbf{y}} \frac{\Gamma\left(\mathbf{y} \mid H_{k}, \hat{\mathbf{u}}_{k}\right)}{\sum_{k^{\prime}=1}^{C} \Gamma\left(\mathbf{y} \mid H_{k^{\prime}}, \hat{\mathbf{u}}_{k^{\prime}}\right)}\right\}\label{equation15}
\end{equation}
其中$\widehat{\mathbf{u_k}}$是$\mathbf{u_k}$的估计值。我们很难得到(\ref{equation14})和(\ref{equation15})的闭合解，但可以用蒙特卡罗方法得到数值近似。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{信号段}\label{sec:SignalSeg}

将归一化似然向量定义为$\xi^N=[\xi^N_1,\dots,\xi^N_C]^T$，其中对应的调制元素为$\xi^N_k=P_{cc}(\mathbf{y}|H_k)$，显然
$\sum_{k=1}^C \xi^N_k = 1$。
因为观测序列中的时刻是独立同分布(\textbf{I.I.D})，所以整个向量可以分割成$S$个连续的子序列，对应的长度为$N_S$，$N=\sum^S_{s=1}N_s$。此外，结合(\ref{equation9})和(\ref{equation12})，$\xi^N_k$可以写为:
\begin{equation}
\begin{aligned}
\xi_{k}^{N} &=\frac{\prod_{s=1}^{S} \Gamma\left(\mathbf{y}^{N_{s}} \mid H_{k}\right)}{\sum_{k^{\prime}=1}^{C} \prod_{s=1}^{S} \Gamma\left(\mathbf{y}^{N_{s}} \mid H_{k^{\prime}}\right)} \\
&=\frac{\prod_{s=1}^{S} \xi_{k}^{N_{s}}}{\sum_{k^{\prime}=1}^{C} \prod_{s=1}^{S} \xi_{k^{\prime}}^{N_{s}}} \\
&=\gamma_{k}\left(\boldsymbol{\xi}^{N_{1}}, \ldots, \boldsymbol{\xi}^{N_{S}}\right)\label{equation16}
\end{aligned}
\end{equation}
其中$\xi^{N_s}$表示分段$s$的归一化似然向量，当$N$较大时有意义，我们将其分成几个分段进行分块计算。更重要的是，(\ref{equation16})表示分段等同于原来的分类任务，分块计算的设计可以灵活可实现。

\section{基于CNN的分类器}\label{CNNClassifier}

我们提出了一种新的端到端DNN使能AMC~\cite{r32}-~\cite{r34}，它与ML-AMC非常接近，并显著提高了推理速度。
\subsection{在AMC中使用CNN的原因}\label{useCNNreason}

假设信道环境是时间不变和频率非选择性的，则观测序列$y$中的元素是独立同分布(\textbf{I.I.D})，$y$的一些特性必须注意：
\begin{itemize}
\item  任意时间间隔内的分段$y^{N_s}$的统计特征是相同的，即:
\begin{equation}
\begin{aligned}
\mathbb{E}\left\{\mathrm{L}\left(\mathbf{y}_{i}^{N_{s}} \mid H_{k}, \mathbf{u}_{k}\right)\right\} &=\mathbb{E}\left\{\mathrm{L}\left(\mathbf{y}_{j}^{N_{s}} \mid H_{k}, \mathbf{u}_{k}\right)\right\}, \\
0 & \leq i, j<N-N_{s},\notag
\end{aligned}
\end{equation}
其中，$\boldsymbol{y}_i^{N_s}=[y_i,\dots,y_{i+N_s−1}]^T$
\item  信号序列$y_n$可以被分成$S$个连续的子序列段，并且似然函数可以表示为$\boldsymbol{L}(\boldsymbol{y_n}|H_k)=\sum^{N_S}_{s=1}\boldsymbol{L}(\boldsymbol{y^{N_s}}|H_k)$。
\item  任意交换$y$中的元素任意次，得到$\boldsymbol{y}$的似然函数为$\boldsymbol{L}(\boldsymbol{y}'|H_k)=\boldsymbol{L}(\boldsymbol{y}|H_k)$。
\end{itemize}

性质1表示信号在时域中均匀分布。具有卷积核的信号段在不同时刻的冲激响应是相同的，因此可以在任何时隙提取或检测特征。性质2证明似然函数是合成的，并且可以由堆叠的级联表示模型来近似。因此，具有局部感受野的CNN比全连接的NN更适用。此外，在给定较大的N的情况下，对于全连接模型，训练过程中的过拟合问题变得更加严重。
性质3表示信号是作为时间序列进行采样的，但它与时间无关，因此不建议使用递归神经网络(RNN)来解决此问题。
 \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen1-2868698-large.pdf}
  \end{overpic}
     \caption{以矢量和标量作为输入的基于CNN的调制分类器的图示。文本‘Conv 12×3，1 \  ReLU’表示在该卷积层中，核数、核大小和步长分别为12、3和1。类似地，文本‘AveragePool 2，1’表示在该平均池化层中，窗口大小和步幅分别为2和1。
     }\label{fig:chen1}
 \end{figure}
\subsection{网络架构}\label{networkarchitecture}

我们设计的CNN-AMC的结构如\figref{fig:chen1}所示，输入由两部分组成：维数$N=1000$的预处理观测序列$\mathbf{y}$和与ML-AMC相同的估计符号信噪比。然而，$\mathbf{y}$是一个原始信号向量，需要进行卷积才能生成更高级别的信息，而SNR只是一个标量，不能卷积。不同类型的信号不能简单地串联起来作为CNN-AMC的输入。取而代之的是使用具有多个输入的混合神经网络。

为了将复矢量$y$变换成实数数据，将实部和虚部分开，然后将其转换成形状$2\times N$。例如，对于第一卷积层$l=1$，输入矢量可以表示为$\boldsymbol{X}^{l−1}=\boldsymbol{y}\in \mathbb{R}^{N_{l−1}\times V^{l−1}}$，其中矢量数量和维数分别为$N_{l−1}=2$和$V_{l−1}=1000$。输出向量的大小列在\figref{fig:chen1}中相应层框的左侧。对于具有$N_{l−1}$输入和$N_l$输出向量的一维卷积层$l$，存在$N_l\times N_{l−1}$卷积核(也称为权重矩阵)和$N_l$偏置，核大小用$k_l$表示。给出上一层的输出向量$\mathbf{x}^{l−1}_i,
i \in \mathcal{N}_{l-1}=\left\{1, \ldots, N_{l-1}\right\}
$当前层的第i个输出向量表示为:
\begin{equation}
\mathbf{x}_{i}^{l}=f\left(\sum_{j=1}^{N_{l-1}} \mathbf{x}_{j}^{l-1} * \mathbf{k}_{i, j}^{l}+b_{i}^{l}\right)\label{equation17}
\end{equation}
其中，采用整流线性单元(RELU)~\cite{r35}作为快速非线性激活函数：$f(x)=max(0,x)$，并且$∗$是卷积操作，在卷积层l中有$N_l\times N_{l−1}\times K_l+N-l$可训练参数。池化层也称为子采样层，它产生输入向量的下采样版本。形式上，我们将第$i$个输出向量设为：
\begin{equation}
\mathbf{x}_{i}^{l+1}=\operatorname{down}\left(\mathbf{x}_{i}^{l}\right)\label{equation18}
\end{equation}

其中，$down(·)$表示子采样函数，并且平均池化~\cite{r36}用于通过计算加窗输入向量的平均值来降低维数。在堆叠卷积层和池化层之后，输出向量被Flatten 层压缩成向量。然后使用Dense 层将该向量编码成包含提取的高层信息的256维向量。对于具有输入矢量$X^{l−1}$的Dense层$l$，输出矢量可以描述为:
\begin{equation}
\mathbf{x}^{l}=f\left(\mathbf{W}^{l} \mathbf{x}^{l-1}+\mathbf{b}^{l}\right)\label{equation19}
\end{equation}
其中$\boldsymbol{W^l}\in \mathbb{R}^{N_1×N_{l−1}}$和$\boldsymbol{b}^l\in \mathbb{R}^{N_1}$分别是权重矩阵和偏置。这是一个全连接层，可训练参数的数量为$N_l\times N_{l−1}+N_l$。然后将该向量与标量信噪比的矢量化表示级联。
在另一Dense层之后，然后，由激活函数为Softmax的输出层产生归一化似然向量，即$\widehat{\xi}^N$。输出$\widehat{\xi}^N = [\widehat{\xi}^N_1,\dots,\widehat{\xi}^N_C]^T$，元素$\widehat{\xi}^N_i$定义为：
\begin{equation}
\hat{\xi}_{i}^{N}=\frac{e^{x_{i}}}{\sum_{j=1}^{C} e^{x_{j}}}\label{equation20}
\end{equation}
其中$x_j$是第$j$个预激活输出。最后，CNN-AMC的表示可以描述如下：
\begin{equation}
\hat{\boldsymbol{\xi}}^{N}=F(\mathbf{y}, \mathrm{SNR} ; \theta)\label{equation21}
\end{equation}
其中$\theta$是网络参数，$F(·)$是基于CNN的分类器的总体函数。在该深层模型中，特征提取由CNN部分自动实现，高层主要处理输入输出数据之间的映射关系。通常，CNN-AMC是目标函数(\ref{equation9})的强大非线性逼近器$F(·)$。完整的训练/测试和仿真源代码可以从\url{https://github.com/mengxiaomao/cnn_amc}下载。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{两步式训练}\label{Two-step}
在一般的训练过程中，CNN-AMC是逐时段训练的，并且在一个时段内，所有训练样本被用来训练CNN-AMC一次。在一个历元内，对整个训练集进行变换，分成大小为$N_b$的批次，逐批次地对模型进行训练。本文采用交叉熵损失函数，将训练批次的损失函数表示为:
\begin{equation}
L(\theta)=-\frac{1}{N_{\mathrm{b}}} \sum_{i=1}^{N_{\mathrm{b}}} \boldsymbol{\xi}_{i}^{N} \log \hat{\boldsymbol{\xi}}_{i}^{N}+\left(1-\boldsymbol{\xi}_{i}^{N}\right) \log \left(1-\hat{\boldsymbol{\xi}}_{i}^{N}\right)\label{equation22}
\end{equation}
其中$\boldsymbol{\xi}_{i}^{N}$表示目标输出。该损失函数通过随机梯度下降(SGD)算法最小化，并且模型参数被迭代地更新为:
\begin{equation}
\theta_{t+1}=\theta_{t}-\eta \Delta L\left(\theta_{t}\right)\label{equation23}
\end{equation}
其中$\eta$表示学习率，操作$\Delta$表示梯度运算符。作为SGD算法的一种变体，本文采用了自适应矩估计(ADAM)~\cite{r37}算法，并采用了缺省设置。ADAM优化器通过对低阶动量的自适应估计，动态地自适应调整$\eta$以加快收敛速度，并被证明是有效的，特别是对于深部模式。记录每个训练时段的参数集$\theta$和验证损失，并定义最优的验证损失最小的CNN-AMC。仿真中可以产生不同信噪比下不同长度的接收观测序列，ML-AMC还可以产生大量的输出数据。

然而，在我们的实验中，由于模型复杂，目标函数复杂，基于CNN的分类器的直接训练具有挑战性。通过大量的训练数据和足够的迭代次数，我们发现损失函数$L(\theta)$不能下降，说明该模型从一开始就停留在高维参数优化上。
虽然最后一项任务很困难，但对于模型来说，学习更简单的函数会容易得多。有一个使用辅助数据训练基于CNN的分类器的简单但有用的技巧。如\figref{fig:chen2}所示，整个培训过程分为两个步骤。
 \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen2-2868698-large.pdf}
  \end{overpic}
     \caption{CNN-AMC的两步训练说明。在步骤1中，所有初级输出元素用橙色圆圈表示，而附加噪声项用灰色圆圈表示。在步骤2中去除高斯噪声的输出元素。步骤1中的输入和输出用$x_I$和$y_I$表示，类似地，步骤2用$x_{II}$和$y_{II}$表示。
     }\label{fig:chen2}
 \end{figure}

\begin{table}[]
\caption{两步训练的仿真参数}
\begin{tabular}{c|c|c}
\hline 训练步骤 & 步骤 1 & 步骤 2 \\
\hline Epoch 数量 & 20 & 240 \\
SNR 范围/dB & {$[0,10]$} & {$[-6,12]$} \\
训练数据量 & 79,200 & 228,060 \\
验证数据量 & 8,800 & 25,340 \\
使用ML分类器的输出 & No & Yes \\
\hline
\end{tabular}
\label{tab:two-step}
\end{table}
\begin{itemize}
\item \textit{预训练}：此步骤旨在帮助CNN-AMC在训练开始时收敛，因此训练数据的规模很小，如\ref{tab:two-step}所示。本步骤中增加了另一个带灰色圆圈的输出项，其对应的类别是零均值的高斯白噪声，而不是其他调制方案。噪声的辅助训练样本量与任何其他调制方案相同，其$I/O$数据用$\mathbf{x}_{aux}$和$\mathbf{y}_{aux}$表示。输出向量被设置为二进制。在仿真测试中，损失函数$L(\theta)$减小，梯度方差变小，训练一段时间后收敛。存储验证损失最小的参数$\theta$，以便在下一次训练中进行模型初始化。
\item \textit{微调}：首先，第一步，CNN-AMC加载存储的参数$\theta$，辅助输出项在最终分类中是冗余的，所以用主输出层替换顶层，然后随机初始化。在该步骤中，仅生成调制方案的样本，并且ML-AMC以归一化似然向量$\xi^N$的形式产生训练数据的输出，这与第一步不同。精心设计的训练数据帮助CNN-AMC逼近ML-AMC，仿真结果表明，使用ML-AMC生成的数据比使用普通二进制标记的$P_c$性能更接近ML-AMC。类似地，验证损失最低的CNN-AMC在以下模拟中被存储、重新加载和测试，详细设置如表\ref{tab:two-step}所示。
\end{itemize}
\begin{table*}[htbp]
\centering
\caption{两步训练综述}
\begin{tabular}{l}
\hline \hline 步骤 1: \\
1) 生成训练数据，并且包括附加高斯噪声的样本。 \\
2) 随机初始化CNN-AMC参数。 \\
3) 训练CNN-AMC，以最小的验证损失存储模型参数。\\
\hline 步骤 2: \\
1) 产生训练数据，并从ML-AMC获得目标输出。 \\
2) 更换CNN-AMC的顶层并随机初始化；从存储器加载其他模型参数。 \\
3) 训练CNN-AMC，以最小的验证损失存储模型参数。 \\
\hline \hline
\end{tabular}\label{tab:2}
\end{table*}
 \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen3-2868698-large.pdf}
  \end{overpic}
     \caption{准确性和损失的培训/验证曲线。X轴表示训练时期。验证和训练曲线用深蓝色和橙色线条绘制，精确度和损耗曲线用实线和虚线表示。
     }\label{fig:chen3}
 \end{figure}
 
 表\ref{tab:2}给出了两步训练方法的总结。在\figref{fig:chen3}中，绘制了完整的两步训练过程中的损失和精度的训练/验证曲线。一般来说，损耗曲线在两个阶段都是下降的，而精度曲线是上升的。在第二步中，训练损失在开始时急剧下降，然后稳步下降。验证损失曲线与训练损失曲线具有相似的趋势。随着历元的增加，训练时间趋于平稳，抖动较小，很难低于0.47，即训练处于饱和区。存储具有最小验证损失的相应$\theta$，并用红点标记步骤2中的最小值。

复平面上的辅助高斯噪声的似然函数是准凸的，这是一种比较直观的逼近函数。从(\ref{equation7})中我们知道，一个观测时刻的条件概率密度函数是具有不同均值和相同方差的高斯分布的加权和。因此，对于已经学习了高斯噪声函数的CNN-AMC来说，对其他调制方案的更复杂的PDF的近似变得更加容易。在学习了符号的近似后，根据(\ref{equation9})，目标函数变得更容易获得。在第一步中，初始收敛问题得到了解决。通过预训练，CNN-AMC在第二步中被进一步训练为尽可能接近最优ML-AMC。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{迁移学习}\label{Transfer Learning}
上述两步训练处理的问题是：给定具有随机参数初始化的CNN-AMC，以及如何训练它以利用给定的训练和验证数据来分类调制方案。事实上，一个训练好的CNN-AMC可以应用于一个特定的任务，例如具有几种候选调制模式的相干环境。一旦场景或调制集改变，CNN-AMC需要用新的训练数据重新训练。

事实上，没有必要在迁移学习的帮助下从一开始就重新启动新的培训过程~\cite{r38}。
如前所述，CNN-AMC的CNN部分从经过预处理的原始观测序列中产生更高层的信息。特征提取可以在不同的任务中普遍使用，并且可以利用一些已经学习到的先验知识或经验而不是纯粹的空白来更容易地进行训练。这里，我们以两个案例为例：
\begin{itemize}
\item \textit{多种调制方式}：在不同的情况下，根据分类任务的要求，增加或删除一些调制方式。除了新的训练数据外，CNN-AMC的输出层需要更改为新的相应类别。
因此，除了最上层的参数外，所有CNN-AMC参数都可以作为初始化加载，而不是随机设置。
\item \textit{不同的环境}：在预处理期间，载波相位补偿包括在相干情况下，但不包括非相干情况。使用提出的两步法可以用新的非相干训练数据来训练CNN-AMC。此外，在一致性情况下学习的基于CNN的分类器可以作为初始模型使用这些数据进行训练，所有的模型参数都是从一致性情况下训练的模型参数中加载的。我们给出了一个使用迁移学习的例子，与之对应的是使用两步训练的例子。如图\ref{fig:chen4}所示，损失曲线的下降速度比使用两步训练的速度快。此外，可以省去训练前的步骤，进一步降低了训练周期的时间成本。
\end{itemize}

虽然迁移学习有助于提高训练效率，但它是通过两步学习来发现的。
 \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen4-2868698-large.pdf}
  \end{overpic}
     \caption{验证数据的丢失和准确度随历元时间的变化曲线。与两步训练算法相比，采用转移学习的算法达到饱和区域所需的时间更短，并且减少了预训练步骤。
     }\label{fig:chen4}
 \end{figure}
  \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen5-2868698-large.pdf}
  \end{overpic}
     \caption{7种调制方式的$P_{cc}(\mathbf{y}|H_k)$曲线，信噪比范围为[−6，10]dB。ML分类器和基于CNN的分类器分别用实线和虚线描述。
     }\label{fig:chen5}
 \end{figure}
\subsection{单位分类器}\label{Unit Classifier}

为了对低信噪比的信号实现高分类精度，序列空间N可以高达几个甚至数万。随着N的增加，需要更多的训练样本来克服过拟合问题，从而使训练变得更加困难。问题是在有限的存储空间和计算资源下，分类及其对应的$N$的训练可能很难，甚至不切实际。另一方面，设计一种输入空间可变的分类器也是一项具有挑战性的工作。

根据前面在第\ref{Two-step}小节中的分析，大$N$的输入$\mathbf{y}$可以被分成具有单位长度的段。将输入信号$\mathbf{y}$为单位输入维数的基于CNN的分类器定义为单位分类器。从理论上讲，在相干环境下，该分段与完全观测分段具有相同的分类性能。更重要的是，单元分类器关于训练、缓存和计算的比例是可调整的。因此，AMC的设计可以灵活地适应硬件，与序列空间$N$无关，也可以对所有的单元分类器并行进行推理。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{仿真结果}\label{simulation}

通过研究，AMC的通用序列空间N从数百到数千不等，并考虑到硬件条件，对观测空间$N=500$(命名为CNN-500)和$N=1000$(命名为CNN1000)的CNN-AMC进行了测试。此外，将ML-AMC与基于9个高阶累积量的基于特征的AMC~\cite{r39}(称为Cu-AMC)进行了比较。采用二阶、四阶和六阶累积量作为特征，采用两层隐层神经网络作为分类器。考虑了几种调制方案。总体正确概率和条件正确概率对信噪比的影响是需要研究的两个主要问题。此外，还必须考虑对估计误差的鲁棒性、推理时间开销和系统存储需求等多个方面。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{相干}\label{coherent}

仿真从相干环境开始，考虑了先验概率相等的7种调制方式，即2PSK、4PSK、8PSK、16QAM、16APSK、32APSK和64QAM。假设接收器具有精确的参数集$\mathbf{u_k}$的知识，包括恒定相位偏移$\theta_c$和SNR，但是需要推断调制方案。除了(\ref{equation6})的功率归一化之外，在预处理周期中用共轭相位偏移来补偿观测序列。
\begin{equation}
\mathbf{y}=\mathbf{y} e^{-j \theta_{c}}\label{equation24}
\end{equation}
结合(\ref{equation7})、(\ref{equation8})和(\ref{equation9})，假设$H_k$下的似然函数由下式显式给出：
\begin{equation}
\Gamma\left(\mathbf{y} \mid H_{k}\right) \propto \frac{1}{M_{k}} \prod_{n=0}^{N-1} \sum_{i=1}^{M_{k}} \exp \left(\frac{\left|x_{n}^{k, i}\right|^{2}-2 \Re \mathfrak{e}\left\{y_{n}^{*} x_{n}^{k, i}\right\}}{N_{0}}\right)
\end{equation}
因此，可以得到归一化似然向量$\xi^N$，并将其与观测序列$y$相结合，可以为CNN-AMC生成大量的训练和验证数据。
  \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen6-2868698-large.pdf}
  \end{overpic}
     \caption{不同序列空间N对信噪比的$P_c$曲线。ML-AMC、CNN-500和CNN-1000分别用实线、点状线和虚线表示。
     }\label{fig:chen6}
 \end{figure}
  \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen7-2868698-large.pdf}
  \end{overpic}
     \caption{集合$\{3^{\circ},6^{\circ},9^{\circ},12^{\circ},15^{\circ}    \}$中不同相移$|\Delta \theta_c|$估计误差的$P_c$曲线。ML和CNN-1000分类器分别用实线和虚线表示。
     }\label{fig:chen7}
 \end{figure}
\begin{itemize}
\item \textit{正确概率}：支持深度学习的AMC比ML-AMC快得多。然而，作为最优AMC的近似值，它在本仿真研究的$P_{CC}(\mathbf{y}|H_K)$和$P_c$方面可能会受到一定程度的性能下降。

 \qquad 首先，7种调制的条件正确概率$P_{CC}(\mathbf{y}|H_K)$曲线如\figref{fig:chen5}($N=1000$)所示。
ML-AMC用实线表示，CNN-1000用虚线绘制。显然，2PSK是我们集合中最容易识别的调制，即使在低信噪比的情况下，它的条件正确概率也是100\%。与其他调制方式相比，4PSK和8PSK更容易识别，当$SNR=2dB$和$SNR=4dB$时，其$P_{CC}(\mathbf{y}|H_K)$分别达到1.00。在高信噪比($SNR=10dB$)下，ML-AMC几乎可以毫无错误地识别出所有的调制方式。

 \qquad 通常，将CNN-1000与ML-AMC进行比较时，$P_{CC}(\mathbf{y}|H_K)$性能略有下降。在某些SNR区域，当使用CNN-1000时，对于某些调制，$P_{CC}(\mathbf{y}|H_K)$高于ML-AMC。例如，在信噪比从−6到−2dB并且调制方案是16QAM的情况下，CNN-1000似乎比ML-AMC更好。这是因为相应的条件概率被高估，代价是某些其他调制方案(例如64QAM)的较低的正确分类概率。
CNN-1000的$P_c$只能无限逼近ML-AMC，不能超过最优值。

 \qquad ML-AMC、CNN-500、CNN-1000和Cu-AMC的总体正确概率曲线如\figref{fig:chen6}所示。
 当$N=500$和$N=1000$时，CNN-AMC和ML-AMC之间大约有1\%到2\%的$P_c$性能下降。此外，当序列空间N为单位输入长度的整数倍时，使用CNN-500和CNN-1000作为单元分类器，当$N=2000$和$N=8000$时，逼近几乎非常接近ML-AMC曲线。仿真结果表明，在精确知道参数集的情况下，基于CNN的分类器能够很好地逼近ML-AMC。另一方面，在相干场景下，它的计算速度比使用7种调制方式的ML使能方法快约150~170倍。
此外，基于CNN的分类器性能优于Cu-AMC，当$P_c>90\%$时，基于特征的分类效果不如CNN-AMCS。更重要的是，实验结果表明，在这种情况下，支持深度学习的方法比基于人工特征提取的方法具有更好的性能。


\item \textit{估计误差}：以前的仿真都是在完全知道固定相位偏移和信噪比这两个参数的前提下给出的。然而，估计误差通常是不可避免的。在这一部分中，对不同程度的估计误差进行了仿真，并采用了变量控制的方法。
  \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen8-2868698-large.pdf}
  \end{overpic}
     \caption{在集合\{0，3，6，9，12\}中不同信噪比的$P_c$曲线，估计误差$\Delta SNR$范围为$[−5,5]$dB。ML-AMC和CNN-1000分别用实线和虚线表示
     }\label{fig:chen8}
 \end{figure}
\qquad 在第一个仿真中，研究了载波相位偏差。由于一般星座图是对称的先验知识，正负偏差对载波相位的影响是等价的。我们使用$|\Delta \theta_c|$表示相位误差，使用$|\Delta \theta_c|\in \{3^{\circ},6^{\circ},9^{\circ},12^{\circ},15^{\circ}    \}$

\qquad 结果如\figref{fig:chen7}所示。一般来说，$P_c$随$|\Delta \theta_c|$的升高而减小。当$|\Delta \theta_c|<6^{\circ}$时，分类性能下降很小。当$|\Delta \theta_c|\in [6^{\circ}，9^{\circ}]$时，ML-AMC明显恶化，当信噪比在$[8，10]dB(|\Delta \theta_c|=9^{\circ})$范围内增大时，ML-AMC更加恶化。当$SNR > 6dB$，$|\Delta \theta_c|=12^{\circ}$时，$P_c$保持在0.86左右。当$|\Delta \theta_c|=15^{\circ}$时，$P_c$甚至不能超过0.7。作为比较，基于CNN的分类器与ML-AMC大致相同，在某些情况下可以执行得更好。在$|\Delta \theta_c|$高达$15^{\circ}$的极端情况下，基于CNN的分类器的$P_c$比ML-AMC的$P_c$要高，这意味着NN使能方法更健壮。

 \qquad 在第二次仿真中，研究了信噪比估计误差对系统性能的影响。该错误可能会导致错误地假设星座点周围的数据集中，并且这些字母表的中心也用有偏差的系数进行归一化。

 \qquad 如\figref{fig:chen8}所示，x轴为−5~5dB的SNR误差，定义为$\Delta SNR=\widehat{SNR}−SNR$，y轴为$P_c$。实SNR$\in \{0,3,6,9,12\}dB$的曲线如下贴上不同颜色和记号的标签。由此可见，分类器对信噪比误差非常敏感，即使是0.5dB的估计偏差也会导致推理性能下降。此外，高估的信噪比比低估的信噪比对分类器的危害更大。我们还注意到，CNN-AMC的曲线比ML-AMC更平滑，在很多情况下，ML-AMC和基于CNN的分类器的$P_c$相差可以超过10\%。在$SNR=12±1.5dB$范围内，ML-AMC的$P_c$仍为1.0，表明性能饱和可以在一定程度上抵抗SNR估计误差。同时，在所有5个比较中，相应的基于CNN的分类器的性能是最差的。我们认为，这种退化可以归因于训练数据不足。
在表\ref{tab:two-step}中，训练数据在[−6，12]dB范围内采样，这意味着包括该范围在内的CNN-AMC没有被训练。

 \qquad 为了使分类器对信噪比误差具有较强的鲁棒性，一种可行的方法是在信噪比上增加噪声的情况下训练CNN-AMC。该方法可以降低分类器对信噪比偏差的敏感度，但以牺牲整体分类性能为代价。
\end{itemize}
  \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen9-2868698-large.pdf}
  \end{overpic}
     \caption{ML-AMC和CNN-500的$P_c$曲线在[−6，6]dB范围内随信噪比变化。
一致性情况下的分类器用浅蓝色线条标绘，非一致性情况下的分类器用海军蓝线条表示。
     }\label{fig:chen9}
 \end{figure}
   \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen10-2868698-large.pdf}
  \end{overpic}
     \caption{ML-AMC和CNN-500在不同观测序列空间$N$下的$P_c$曲线与信噪比的关系。ML-AMC用点状线绘制，使用信号段时用实线表示。CNN-AMC用虚线描绘，并使用单元分类器。
     }\label{fig:chen10}
 \end{figure}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{不相干}\label{incoherent}

在非合作的情况下，相干环境通常是不可访问的，需要在接收端使用盲相位估计算法。在以往的仿真中，当相位估计误差$|\Delta \theta_c|$大于$6^{\circ}$时，CNN-AMC的分类性能会受到很大的影响，因此最终的分类精度很大程度上取决于所采用的估计算法。在本小节中，假设$\theta_c$是在$[0，2\pi]$范围内均匀分布的随机变量。
类似地，可以如下获得似然函数:
\begin{equation}
\begin{aligned}
\Gamma\left(\mathbf{y} \mid H_{k}\right) \propto & \mathbb{E}_{\theta_{c}}\left\{\frac{1}{M_{k}} \prod_{n=0}^{N-1} \sum_{i=1}^{M_{k}} \exp \left(-\frac{\left|x_{n}^{k, i}\right|^{2}}{N_{0}}\right)\right.\\
&\left.\times \exp \left(\frac{2 \Re \mathfrak{e}\left\{y_{n}^{*} x_{n}^{k, i} e^{j \theta_{c}}\right\}}{N_{0}}\right)\right\} \\
=& \frac{1}{M_{k}} \int_{0}^{2 \pi} \prod_{n=1}^{N} \sum_{i=1}^{M_{k}}\left[\exp \left(-\frac{\left|x^{k, i}\right|^{2}}{N_{0}}\right)\right.\\
&\left.\times \exp \left(\frac{2 \Re \mathfrak{e}\left\{y_{n}^{*} x^{k, i} e^{j \theta_{c}}\right\}}{N_{0}}\right)\right] d \theta_{c}
\end{aligned}\label{equation26}
\end{equation}

事实上，零阶第一贝塞尔函数可以用来简化(\ref{equation26})，但由于其复杂性，在数值计算中不建议使用它。在替换中，通过将积分区间$[0,2\pi)$分成相同长度的子区间，可以借助Simpson规则~\cite{r20}充分逼近$\theta_c$上的期望值。利用复平面上星座点的对称性，可以将距离缩短到$[0,\pi)$。在$\theta_c$上的附加积分使得计算总量是相干环境下的几倍，并且计算时间开销与子区间数成正比。

为了减少总计算量，调制集中仅包括2PSK、4PSK、8PSK和16QAM。此外，只有CNN-500使用观测空间$N=500$的非相干数据进行再训练。在预处理期间，不需要(\ref{equation24})中的相位补偿步骤。
\begin{itemize}
\item \textit{相干VS非相干}：ML-AMC、CNN-AMC和Cu-AMC正确分类的总体概率如图9所示。可以看出，在非相干场景下，CNN-AMC的近似非常接近最优分类，而且它的性能也优于Cu-AMC。作为比较，三种分类器在相干场景下的总体正确概率用浅蓝色线条表示。在相干情况下，CNN-AMC的$P_c$性能略好于Cu-AMC，但在非相干情况下，CNN-AMC的优势更加明显。要达到$P_c=90\%$，非相干分类器所需的信噪比可达1.6dB，比相干分类器高约1.0dB。一般来说，相干分类器和非相干分类器之间的性能差距随着信噪比的增加而逐渐缩小。当SNR=4dB时，两种分类器的正确判决概率均为100\%。
   \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen11-2868698-large.pdf}
  \end{overpic}
     \caption{在$SNR=−6dB$范围内，左侧ML-AMC和右侧CNN-AMC的可视化混淆矩阵。每个目标类用10000个样本进行测试，确切的数量和比例用蓝色和黄色的方块给出。在灰度列块、灰度行块和白块中，分别给出了查全率、查准率和准确率的值。
     }\label{fig:chen11}
 \end{figure}
 非相干CNN-500也在观测空间$N\in \{500,1000,2000\}$上进行了测试，\figref{fig:chen10}用虚线将ML-AMC视为最佳对比度。文本“切片”表示使用单位分类器计算似然函数，使用分段的ML-AMC用实线表示。从理论上讲，在非相干环境下，观测信号的分块计算并不是最优的。从图中可以看出，当分段数$N_s$为2和4时，分别约有0.2dB和0.4dB的$P_c$损耗。另一方面，单元CNN-500的近似也非常接近采用不同输入间隔分段的ML-AMC。当$N\in \{1000,2000\}$时，在$SNR=2dB$范围内，分类器的总体正确概率达到10 0\%。虽然单元分类器在性能上有一定损失，但在非相干情况下作为次优方法是可行的。此外，具有特定输入空间的新的CNN-AMC可以被训练成最优方法的近似值。
\item \textit{条件正确概率}：通过对ML-AMC和CNN-AMC四种条件正确概率结果的初步分析，发现基于CNN的分类器在高信噪比区域与ML-AMC的近似程度相当接近，但在$SNR<2dB$时不能很好地匹配。

\qquad 为了进一步研究条件正确概率性能，在$SNR=−6dB$的范围内,\figref{fig:chen11}显示了两个AMC的混淆矩阵。每行表示AMC的预测类中的实例，每列表示目标调制类中的实例。比较两个AMC的召回率和查准率，主要区别在于4PSK和8PSK。4PSK和8PSK都是相移键控，在复平面上邻接点之间的距离很小。因此，这两种调制方式很容易相互误分，特别是在低信噪比的情况下。损失函数被设置为最大化总体正确概率，并且即使$P_c$在\figref{fig:chen9}中接近近似，在这种情况下也不能保证条件正确概率的近似。
\end{itemize}

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{平坦衰落信道}\label{FlatFaddingCH}
研究了非频率选择性慢衰落信道。因此，假设幅度$a$和载波相位$\theta_c$在整个观测周期内为常数。载波相位的$PDF$与\ref{incoherent}中的相同。此外，假设振幅$a$的$PDF$服从下式给出的瑞利分布:
\begin{equation}
p(a)=\frac{2 a}{\sigma_{a}^{2}} \exp \left(-\frac{a^{2}}{\sigma_{a}^{2}}\right), \quad a \geq 0
\end{equation}

其中$σ^2_a=\mathbb{E}[a^2]$。除了预处理的观察序列$y$之外，CNN-AMC仍然需要符号$SNR$的知识。似然函数表示为:
\begin{equation}
\begin{aligned}
\Gamma\left(\mathbf{y} \mid H_{k}, a\right) \propto & \mathbb{E}_{\theta_{c}}\left\{\frac{1}{M_{k}} \prod_{n=1}^{N} \sum_{i=1}^{M_{k}} \exp \left(-\frac{\left|a x^{k, i}\right|^{2}}{N_{0}}\right)\right.\\
&\left.\times \exp \left(\frac{2 \Re \mathfrak{e}\left\{y_{n}^{*} a x^{k, i} e^{j \theta_{c}}\right\}}{N_{0}}\right)\right\} \\
=& \frac{1}{M_{k}} \int_{0}^{\pi} \prod_{n=1}^{N} \sum_{i=1}^{M_{k}}\left[\exp \left(-\frac{\left|a x^{k, i}\right|^{2}}{N_{0}}\right)\right.\\
&\left.\times \exp \left(\frac{2 \Re e^{*}\left\{y_{n}^{*} a x^{k, i} e^{j \theta_{c}}\right\}}{N_{0}}\right)\right] d \theta_{c}
\end{aligned}
\end{equation}
   \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen12-2868698-large.pdf}
  \end{overpic}
     \caption{$P_c$与$\bar{SNR}$在范围[−18,18]dB内的关系。每个天线的观测空间$N=500$，1,2,4个天线的曲线分别用浅蓝色，深蓝色和绿色线表示
     }\label{fig:chen12}
 \end{figure}
    \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen13-2868698-large.pdf}
  \end{overpic}
     \caption{在相干环境中($N=500$)对各种AMC进行基线测试。$P_c$对$SNR$的曲线图。
     }\label{fig:chen13}
 \end{figure}
     \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen14-2868698-large.pdf}
  \end{overpic}
     \caption{在非相干环境中($N=500$)对各种AMC进行基线测试。$P_c$对SNR的曲线图。
     }\label{fig:chen14}
 \end{figure}
仿真中考虑了单输入多输出(SIMO)的情况，其中一台发射机和一台接收机具有$N_a$个天线。假设这些通道彼此独立。具有多个观测值的似然函数给出如下:
\begin{equation}
\begin{array}{cc}
\Gamma\left(\left\{\mathbf{y}_{1}, \ldots, \mathbf{y}_{N_{a}}\right\} \mid H_{k},\left\{a_{1}, \ldots, a_{N_{a}}\right\}\right)=\\ \prod_{u=1}^{N_{a}} \Gamma\left(\mathbf{y}_{u} \mid H_{k}, a_{u}\right)
\end{array}
\end{equation}
这忽略了发送的码元序列对于所有天线都相同的事实。形式上，一个信号源的多个观测过程类似于CNN-AMC(\ref{equation16})中的信号段。

在我们的仿真中，我们研究了使用1，2，4天线的接收机，从而测试了整体分类性能与平均符号信噪比(用信噪比表示)的性能。如\figref{fig:chen12}所示，CNN-500的曲线如图所示，ML-AMC被视为它的对应物。可以看出，在平坦Raleigh衰落信道中，当$\Bar{SNR}=4dB$时，单天线接收机的总体正确概率$P_c=90\%$。
相比之下，具有2个和4个天线的接收机所需的信噪比分别约为−1dB和−3dB。当$SNR=2dB$时，在4个天线的情况下几乎没有分类错误。结合\figref{fig:chen6}和图\figref{fig:chen9}的性能，CNN-AMC的性能总体上可以达到更接近于最优的结果,相较Cu-AMC而言。

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{基于特征的方法}\label{FeatureBased}
除了前面提到的基于神经网络的方法之外，本小节还考虑了其他一些分类器，包括决策树分类器(DTC)、多项式支持向量机(SVM)和径向基函数(RBF)核的支持向量机(SVM-Poly)和支持向量机-RBF(SVM-RBF)。将\ref{coherent}中的数据集($N=500$)作为相干环境中的基线测试。如\figref{fig:chen13}所示，CNN-AMC和SVM-RBF的总体$P_c$非常相似，而基于神经网络的AMC与这两者相比略有$P_c$性能损失。DTC分类器的分类正确率最低。

然后用\ref{incoherent}中的非相干基线数据集($N=500$)测试这些AMC。结合\figref{fig:chen13}，DTC分类器的正确分类性能仍然是最差，而基于支持向量机的方法在\figref{fig:chen14}中并不稳定。在基于特征的方法中，只有基于神经网络的AMC在相干和非相干环境下都能获得与CNN-AMC最接近的$P_c$性能，这就是为什么在以往的仿真中它被用作Cu-AMC的原因。此外，CNN-AMC的$P_c$性能略好于基于神经网络的AMC。


\begin{table}[htbp]
\centering
\caption{相干环境下分类器的平均推理时间开销}
\begin{tabular}{c|c|c|c|c}
\hline$N$ & 500 & 1000 & 2000 & 8000 \\
\hline ML-AMC/ms & $10.8$ & $21.3$ & $42.6$ & $164.5$ \\
CNN-500/ms & $0.0727$ & $0.146$ & $0.288$ & $1.15$ \\
CNN-1000/ms & $-$ & $0.127$ & $0.246$ & $1.02$ \\
\hline
\end{tabular}\label{tab:3}
\end{table}
 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{复杂性\&实施}\label{Compimple}

在这一节中，我们讨论了ML-AMC和CNN-AMC的复杂性分析和实现。MLAMC不需要内存，而且CNN500和CNN-1000的规模很小，在N=500和N=1000的情况下分别只占用3.4MB和6.3MB的容量。
仿真平台为：CPU intel i7-6700k和GPU Nvidia GTX-960。ML-AMC是用Matlab实现的，主要利用了CPU。而CNN-AMC则运行在搭载GPU的Python平台上。
\begin{itemize}
    \item \textit{相干}：表\ref{tab:3}列出了\ref{coherent}中三个分类器在不同输入空间下的每次推理的时间成本。单位时间成本定义为每个推理的时间成本除以$N$，ML-AMC的单位时间成本为20.8$\mu s$，CNN-500的单位时间成本为0.144$\mu s$，CNN-1000的单位时间成本为0.127$\mu s$。在这种情况下，基于神经网络的方法使计算速度提高了两个数量级以上。更具体地说，CNN-500和CNN-1000分别比ML-AMC快150倍和170倍。
    \begin{table*}[htbp]
\centering
\caption{四种调制方案分类的平均时间开销}
\begin{tabular}{c|c|c|c|c}
\hline \multirow{3}{*}{ Coherent } & $N$ & 500 & 1000 & 2000 \\
\cline { 2 - 5 } & $\mathrm{ML}-\mathrm{AMC} / \mathrm{ms}$ & $3.08$ & $6.07$ & $11.8$ \\
& $\mathrm{CNN}-500 / \mathrm{ms}$ & $0.0775$ & $0.155$ & $0.309$ \\
\hline \multirow{3}{*}{ Incoherent } & $N$ & 500 & 1000 & 2000 \\
\cline { 2 - 5 } & $\mathrm{ML}-\mathrm{AMC} / \mathrm{ms}$ & 138 & 271 & 528 \\
& $\mathrm{CNN}-500 / \mathrm{ms}$ & $0.0775$ & $0.155$ & $0.309$ \\
\hline
\end{tabular}\label{tab:4}
\end{table*}
    \begin{table*}[htbp]
\centering
\caption{四种调制方案分类的平均时间开销}
\begin{tabular}{c|c|c}
\hline & ML-AMC & CNN-AMC \\
\hline Coherent & $k_{\mathrm{ml}} O\left(N \sum_{k=1}^{C} M_{k}\right)$ & $k_{\mathrm{cnn}} O(N)$ \\
\hline Incoherent & $k_{\mathrm{ml}} O\left(N N_{\theta_{c}} \sum_{k=1}^{C} M_{k}\right)$ & $k_{\mathrm{cnn}} O(N)$ \\
\hline Flat fading channel & $k_{\mathrm{ml}} O\left(N N_{\theta_{c}} N_{a} \sum_{k=1}^{C} M_{k}\right)$ & $k_{\mathrm{cnn}} O\left(N N_{a}\right)$ \\
\hline
\end{tabular}\label{tab:5}
\end{table*}
\item \textit{非相干}：在非相干情况下，ML-AMC需要额外的载波相位积分。理论上，单位时间开销约为相干场景下的$N_{θ_c}$倍，其中$N_{θ_c}$为子区间数。在\ref{incoherent}中进行仿真测试后，在相干和非相干场景下,表\ref{tab:5}中列出的单位时间成本分别为6.01$\mu s$和268$\mu s$。
后者约为相干变量的44.7倍，近似等于所采用的变量值$N_{θ_c}=45$。

 另一方面，CNN使能方法的单位时间成本只与模型的体系结构有关。除最上层外，7种调制方式和4种调制方式的CNN-500的结构相同，因此表\ref{tab:4}中的CNN-500的平均时间开销与表\ref{tab:3}中的相应方案非常相似，而且基于CNN的方法的平均时间开销与通信环境无关，其单位时间开销为0.155$\mu s$，因此在此仿真情况下，在相干和非相干环境下，CNN-500分别比ML-AMC快约40倍和1,700倍。

 对于ML-AMC和CNN-AMC，计算量都随着观测空间$N$的增加而线性增长。本文提出的CNN-AMC算法的计算复杂度主要由网络结构决定，调制方式和信道环境对总计算量的影响可以忽略不计。相比之下，ML-AMC受到这些问题的影响很大。表\ref{tab:5}列出了在我们的三个模拟环境中两个AMC的计算复杂度。符号$k_{m1}$和$k_{cnn}$是相应AMC的系数。
根据表\ref{tab:5}，直观地看出CNN-AMC算法的计算复杂度低于ML-AMC算法。实际上，CNN-AMC的$k_{cnn}$的系数比ML-AMC的系数大得多，ML-AMC的总计算量在相干场景下可以更小。但是，由于并行处理，CNN-AMC的每个推理在实现时的时间开销可以大大降低。可以并行实现一层不同核或神经元的输出，也可以同时处理不同接收序列的分类。另一方面，ML-AMC的计算复杂度容易受到信道环境和调制集的影响。相反，所提出的CNN-AMC的每次推理的时间开销是不变的，作为一种具有强大和冗余逼近能力的深度学习模型。这表明CNN-AMC的性能优于ML-AMC，特别是在更复杂的环境中。
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{结论}\label{conclusion}

本文提出了一种基于深度学习方法的自动调制分类方法。作为一种并行计算模型，提出了一种基于深度学习的分类器CNN-AMC，作为ML-AMC的近似。CNN-AMC自动从长符号率序列中提取特征，还需要SNR作为输入。引入了相应的两步训练过程和转移学习，分别解决了具有挑战性的训练任务，提高了再培训效率。此外，为了灵活地适应不同的观测序列维数，还提出了一种单元分类器。仿真结果表明，基于CNN的分类器性能优于基于特征的分类器，且最接近最优ML-AMC。此外，CNN-AMC对载波相位偏差和信噪比估计误差具有一定的鲁棒性。通过利用并行计算，基于深度学习的方法在平均分类速度方面比ML-AMC快约40到1700倍，具体取决于给定的场景。在未来的工作中，我们将重点研究基于CNN的分类器对不同偏差的健壮性，以及在复杂的非合作场景中的应用。
\paragraph{说明.} 本文为发表在IEEE TVT 的Automatic Modulation Classification:A Deep Learning Enabled Approach的中文翻译版，原作者为Fan Meng, Peng Chen, Lenan Wu, Xiaobin Wang。合肥工业大学计算机与信息学院惠英哲为本项工作完成者，作为其学士毕业设计的外文翻译工作。

{\small
\bibliographystyle{ieeetr} 
\bibliography{AMC-CN}

}

% \end{CJK*}
\end{document}
