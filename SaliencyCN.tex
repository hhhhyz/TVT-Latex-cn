%!TEX program=xelatex

% 碰到Windows版本提示Fandol字体，可以在命令行中以管理员权限执行：tlmgr update -self -all
%\documentclass[review]{cvpr}
\documentclass[final]{cvpr}

\usepackage[UTF8]{ctex}

%\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfigure}
\usepackage{overpic}
\usepackage{enumitem}
\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}


\usepackage[pagebackref=true,breaklinks=true,colorlinks,bookmarks=false]{hyperref}


%\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{159} % *** Enter the CVPR Paper ID here
\def\confYear{CVPR 2020}
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

\newcommand{\cmm}[1]{\textcolor[rgb]{0,0.6,0}{CMM: #1}}
\newcommand{\todo}[1]{{\textcolor{red}{\bf [#1]}}}
\newcommand{\alert}[1]{\textcolor[rgb]{.6,0,0}{#1}}

\newcommand{\IT}{IT\cite{98pami/Itti}}
\newcommand{\MZ}{MZ\cite{03ACMMM/Ma_Contrast-based}}
\newcommand{\GB}{GB\cite{conf/nips/HarelKP06}}
\newcommand{\SR}{SR\cite{07cvpr/hou_SpectralResidual}}
\newcommand{\FT}{FT\cite{09cvpr/Achanta_FTSaliency}}
\newcommand{\CA}{CA\cite{10cvpr/goferman_context}}
\newcommand{\LC}{LC\cite{06acmmm/ZhaiS_spatiotemporal}}
\newcommand{\AC}{AC\cite{08cvs/achanta_salient}}
\newcommand{\HC}{HC-maps }
\newcommand{\RC}{RC-maps }
\newcommand{\Lab}{$L^*a^*b^*$}
\newcommand{\mypara}[1]{\paragraph{#1.}}

\graphicspath{{figures/}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}

\begin{document}
% \begin{CJK*}{GBK}{song}

\renewcommand{\figref}[1]{图\ref{#1}}
\renewcommand{\tabref}[1]{表\ref{#1}}
\renewcommand{\equref}[1]{式\ref{#1}}
\renewcommand{\secref}[1]{第\ref{#1}节}
\def\abstract{\centerline{\large\bf 摘要} \vspace*{12pt} \it}

%%%%%%%%% TITLE

\title{自动调制分类：一种支持深度学习的方法\thanks{本文为IEEE TVT发表的对应文献的中文翻译版。}}

\author{Fan Meng$^{1}$, Peng Chen$^{1}$, Member, IEEE, Lenan Wu$^{1}$, and Xiaobin Wang$^{2}$, Fellow, IEEE \\
    $^{1}$ Southeast University \quad \quad
    $^2$ Western University   \quad \quad \\
}

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
自动调制分类(AMC)在民用和军事应用中都起着至关重要的作用，本文通过深度学习的方法对其进行了研究。传统的AMC可以分为基于最大似然(ML)的AMC(ML-AMC)和基于特征的AMC。然而，ML-AMCS的计算复杂度较高，其实际应用难度较大，人工提取的特征需要专家知识。为此，本文提出了一种基于端到端卷积神经网络(CNN)的自适应AMC(CNN-AMC)方法，该方法自动从长符号率观测序列中提取特征和估计的信噪比(SNR)。对于CNN-AMC，采用单元分类器来适应不同的输入维度。针对CNN-AMC直接训练模型复杂、任务复杂的问题，提出了一种新的两步训练方法，并引入了迁移学习来提高再训练的效率。在不同的场景下考虑了不同的数字调制方案，仿真结果表明CNN-AMC的性能优于基于特征的方法，并且更接近最优ML-AMC。此外，CNN-AMC对载波相位偏差和信噪比估计误差具有一定的鲁棒性。在并行计算的情况下，基于深度学习的方法在推理速度方面比ML-AMC快40到1700倍。\\ 
{\bf\emph{ 关键词——\ 自动调制分类；卷积神经网络；深度学习；两步训练 }\rm}
\end{abstract}





%%%%%%%%% BODY TEXT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{引言}\label{sec:Introduction}
在实现最大可能的传输速率时，自适应调制方案被采用在民用无线通信系统中以达到充分利用时变信号。在这种情况下，在通信过程中使用的特定调制方案的信息必须由发射机通过网络协议与接收机共享，代价是协议开销。当接收器具有调制方案识别能力时，可以消除这种开销。另一方面，许多军事应用也需x分类(AMC)~\cite{r1}~\cite{r2}对于实现自动接收机配置、干扰抑制和频谱管理的民用和军用应用都具有重要意义。

传统的基于特征的AMC算法~\cite{r3}、~\cite{r4}可以通过数据预处理、特征提取和分类决策三个步骤来实现。由于良好的统计特征能够以较低的复杂度提供稳健的性能~\cite{r5}，基于特征提取的AMC方法已经在许多已有的文献~\cite{r6}、~\cite{r7}中得到了研究，如循环特征~\cite{r8}、高阶矩和累积量~\cite{r9}-~\cite{r11}、小波变换~\cite{r12}、~\cite{r13}等。在分类决策中，传统的分类器包括随机森林~\cite{r14}、多层感知器~\cite{r15}、~\cite{r16}和支持向量机(SVM)~\cite{r17}、~\cite{r18}。这些AMC是不同特征提取算法和分类的组合，需要专家知识和人工设计

文献~\cite{r19}也研究了基于最大似然(ML)的AMC方法，其中ML-AMC已经被证明在使用数学信道模型的场景中取得了最优的性能。与基于特征的AMC不同，ML-AMC计算所有候选调制的似然函数，并选择似然值最大的调制方案，作为端到端模块。
然而，较高的计算复杂度使得低成本的实时实现变得极其困难~\cite{r20}。此外，在复杂的环境下，ML-AMC的精确概率是很难实现的。

在基于特征的AMC中，机器学习方法只是作为特征和多假设之间的映射函数。为了避免特征工程，深度学习方法~\cite{r21}近年来在算法设计和硬件实现方面发展迅速~\cite{r22}，它可以学习比浅层学习方法更复杂的函数。深度学习的快速发展已经在通信领域取得了一些成功的应用~\cite{r23}-~\cite{r25}，包括调制分类。在~\cite{r26}中，原始信号的眼图用作基于Lenet-5的分类器~\cite{r27}的输入，并巧妙地重新结合图像识别领域的研究，提出了AMC问题。文献~\cite{r28}、~\cite{r29}提出了一种用于无线电调制识别的卷积网络，并用RML2016.10b数据集进行了仿真，结果表明该方法的分类精度高于专家特征工程方法。然后在~\cite{r30}中，基于LSTM的AMC被证明在小尺度或中尺度上优于CNN模型的过采样接收信号。文献~\cite{r31}对长符号率信号进行了研究，堆叠式自动编码器在增加仿真次数的情况下取得了很好的效果。

本文提出了一种基于深度神经网络(DNN)的自适应AMC算法，该算法能够在低信噪比的情况下自动学习从长符号率信号中提取特征，并采用卷积神经网络(CNN)实现这种自适应自适应算法。
CNN-AMC可以在性能损失最小的情况下接近ML-AMC，但速度有显著提高。MLAMC被用作我们提出的CNN-AMC的基准，并且还可以生成不同场景下的训练数据。这项工作的贡献概括如下：
\begin{itemize}
\item 直接利用低信噪比的长符号率信号，提出了一种基于深度CNN的端到端可训练AMC。与基于特征的AMC不同，CNNAMC自动学习从原始信号中提取特征，简化AMC设计。此外，CNN-AMC对ML-AMC进行了近似，相关处理可以并行完成，加快了推理速度。
\item 针对CNN-AMC的直接训练问题，我们提出了一种包括预训练和微调的两步训练方法。对于不同调制方式和通信信道场景的不同组合，还提出了转移学习来进一步提高再训练效率。
\item 通过将经过长预处理的观测序列分成单元大小的信号段，引入单元输入维数以适应不同的输入维数，并并行实现分块计算。 
\end{itemize}

本文的其余部分组织如下。第二节概述了经典决策理论框架下的调制分类问题。第三节给出了具有多输入特征的CNN-AMC的结构，以及相应的两步训练、迁移学习和单元分类器。然后，在第四节中，对所提出的CNN-AMC、ML-AMC和基于特征的方法在不同环境下的信噪比进行了测试，并对仿真结果进行了分析。
第五节给出了结论和讨论。


% \begin{figure}[t!]
%   \begin{overpic}[width=\columnwidth]{teaser.pdf}
%     \end{overpic}
%     \caption{给定输入图像(上)，可以通过全局对比度分析得到高分辨率的视觉显著性图(中)。
%          这种视觉显著性图可以进一步被用来获取感兴趣物体区域(下)。
%     }\label{fig:teaser}
% \end{figure}


% \begin{figure*}[t!]
%   \begin{overpic}[width=\textwidth]{all_comparisons.pdf} \small
%   \put(0.3,0){(a)~输入图}
%   \put(10,0){(b)~\IT}
%   \put(18.3,0){(c)~\MZ}
%   \put(27.5,0){(d)~\GB}
%   \put(37,0){(e)~\SR}
%   \put(46.5,0){(f)~\AC}
%   \put(55.5,0){(g)~\CA}
%   \put(65,0){(h)~\FT}
%   \put(73.5,0){(i)~\LC}
%   \put(84,0){(j)~HC}
%   \put(93,0){(k)~RC}
%   \end{overpic}
%   \caption{通过现有最先进的一组方法(b-i)，以及我们提出的HC方法(j)和RC方法(k)
%       得到的视觉显著性图。其中的许多方法更加强调边或者只能得到低分辨率的显著性图。
%       \figref{fig:VisualComparison}和我们的项目主页中有更多的结果。
%   }\label{fig:cmp1vAll}
% \end{figure*}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{系统模型}
\label{sec:SystemModel}
\subsection{问题表述}
\label{sec:Problemfor}
在不损失一般性的情况下，接收基带信号的复包络可以表示为:
\begin{equation}
y(t)=x\left(t ; \mathbf{u}_{k}\right)+v(t)\label{equation1}
\end{equation}
其中$v(t)$是具有零均值和双边功率谱密度为$N_0/2$的复加性高斯白噪声(AWGN)。无噪声信号$x\left(t ; \mathbf{u}_{k}\right)$通过综合形式被建模为:
\begin{equation}
x\left(t ; \mathbf{u}_{k}\right)=a e^{j\left(2 \pi \Delta f t+\theta_{c}\right)} \sum_{n=0}^{N-1} x_{n}^{k, i} g\left(t-n T_{s}-\varepsilon T_{s}\right)\label{equation2}
\end{equation}
其中，$\mathbf{u}_{k}$定义为多维参数集，其中一串未知信号和信道变量在第$k$次调制方案下是随机或确定的，并且其被给出为:
\begin{equation}
\mathbf{u}_{k}=\left\{a, \theta_{c}, \Delta f,\left\{x^{k, i}\right\}_{i=1}^{M_{k}}, h(t), \varepsilon\right\}\label{equation3}
\end{equation}
在(~\ref{equation2})和(~\ref{equation3})中使用的符号如下所示：
\begin{itemize}
\item $a$，信号幅度。
\item $\theta_{c}$，由传播延迟以及初始载波相位引入的固定相位偏移。
\item $N$，接收序列中的符号数，即观测序列空间。
\item $x^{k, i}$，第$k$调制方案下的第$i$星座点，其中符号均匀分布，
$i \in\left\{1, \ldots, M_{k}\right\}, k \in\{1, \ldots, C\}$，
标记$M_k$分别表示使用该调制的符号数目，$C$是候选等概率调制方案的数目。调制的码元通常用单位功率进行归一化。
\begin{equation}
\frac{1}{M_{k}} \sum_{i=1}^{M_{k}}\left|x^{k, i}\right|^{2}=1\label{equation4}
\end{equation}
此外，数据块中的符号被假定为独立且相同的分布(\textbf{I.I.D})。
\item $\Delta f$，去除载波后的剩余载波频率。
\item $T_s$，符号间隔。
\item $g(t)$，剩余通道$h(t)$和脉冲整形函数$p(t)$的复合效应。它由方程$g(t)=h(t)∗p(t)$表示，其中$∗$是卷积算子。通常，$p(t)$是持续时间$t_s$的升余弦根脉冲函数。
\item $\varepsilon$，发送器和接收器之间的时序偏移的归一化历元，$0≤ε<1$。
\end{itemize}

本文将接收信号表示为采样复数基带表示，并考虑了$\frac{E_s}{N_0}$的信噪比。以符号间隔$t_s$采样的离散观测序列可以写为$\mathbf{y}=[y_0，\dots，y_{N−1}]^T$，其中上标$T$是转置运算符。接收信号的每个采样时刻都是接收端脉冲整形匹配滤波器的输出，可以表示为:
\begin{equation}
y_{n}=\frac{1}{T_{s}} \int_{n T_{s}}^{(n+1) T_{s}} y(t) p\left(t-n T_{s}\right) d t \label{equation5}
\end{equation}
在盲分类的情况下，发送信号的脉冲形状$p(t)$对于接收器是未知的。类似地，瞬时$y_n$通常归一化为单位功率。
\begin{equation}
y_{n}=\frac{y_{n}}{\sqrt{\frac{1}{N} \mathbf{y}^{H} \mathbf{y}}}\label{equation6}
\end{equation}
其中上标$H$是共轭转置运算符。在我们的分类器中，需要估计符号信噪比，这相当于既知道$a$又知道$N_0$。
假设$H_k$下$y_n$的条件概率密度函数(\bf PDF\rm)可以表示为:
\begin{equation}
\begin{aligned}
&p\left(y_{n} \mid H_{k}, \mathbf{u}_{k}\right) =\sum_{i=1}^{M_{k}} \frac{1}{M_{k} \sqrt{\pi N_{0}}} \exp \left(\frac{\left|y_{n}-x_{n}^{k, i}\left(\mathbf{u}_{k}\right)\right|^{2}}{N_{0}}\right) \\
& \propto \frac{1}{M_{k}} \sum_{i=1}^{M_{k}} \exp \left(\frac{\left|x_{n}^{k, i}\right|^{2}-2 \Re \mathfrak{e}\left\{y_{n}^{*} x_{n}^{k, i}\left(\mathbf{u}_{k}\right)\right\}}{N_{0}}\right)
\end{aligned}\label{equation7}
\end{equation}

参数集$\mathbf{u_k}$中的元素是具有已知\textbf{PDFs}的确定性或随机变量。因此，可以通过取(\ref{equation7})的统计平均值来获得观测序列的边际密度函数，并且它被给出为:
\begin{equation}
\Gamma\left(y_{n} \mid H_{k}\right)=\mathbb{E}_{\mathbf{u}_{k}}\left\{\Gamma\left(y_{n} \mid H_{k}, \mathbf{u}_{k}\right)\right\}\label{equation8}
\end{equation}
其$\mathbb{E}\{·\}$是期望运算符。

假设信道环境变化缓慢，设置$\mathbf{u_k}$中的参数在整个观察期内是静态的。此外，归一化历元$\varepsilon=0$且噪声为白色，因此$y$中的元素服从独立且相同的分布(\textbf{I.I.D}),函数$p(y_n|h_k，u_k)$表示时刻$n$处单次观测的概率密度，整个观测序列的联合似然函数可描述为:
\begin{equation}
\Gamma\left(\mathbf{y} \mid H_{k}\right)=\prod_{n=0}^{N-1} p\left(y_{n} \mid H_{k}\right)\label{equation9}
\end{equation}
数值计算中的浮点溢出可以由一个大的$N$引入，因此通常采用(\ref{equation9})的等效对数形式，它由下式给出:
\begin{equation}
\mathrm{L}\left(\mathbf{y} \mid H_{k}\right)=\ln \Gamma\left(\mathbf{y} \mid H_{k}\right)\label{equation10}
\end{equation}
根据最大似然准则，在给定观测序列$y$的情况下，最终选择与第$k$个调制方案对应的最可能的假设$H_k$，其中具有最大似然值:
\begin{equation}
\widehat{k}=\arg \max _{k \in\{1, \ldots, C\}} \Gamma\left(\mathbf{y} \mid H_{k}\right)\label{equation11}
\end{equation}
其中，最高标记($\widehat{.}$)表示估计值。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{正确概率}\label{sec:CorrectPro}

条件正确概率$P_{cc}(\mathbf{y}|H_K)$定义为:
\begin{equation}
P_{\mathrm{cc}}\left(\mathbf{y} \mid H_{k}\right)=\frac{\Gamma\left(\mathbf{y} \mid H_{k}\right)}{\sum_{k^{\prime}=1}^{C} \Gamma\left(\mathbf{y} \mid H_{k^{\prime}}\right)}\label{equation12}
\end{equation}
因此，总体正确概率$P_c$被表示为:
\begin{equation}
P_{\mathrm{c}}=\frac{1}{C} \sum_{k=1}^{C} P_{\mathrm{cc}}\left(\mathbf{y} \mid H_{k}\right)\label{equation13}
\end{equation}
观测序列$\mathbf{y}$的分布为$f_\mathbf{Y}(\mathbf{y})$，然后结合(\ref{equation12})和(\ref{equation13})，$P_c$的期望值由下式给出
\begin{equation}
\begin{aligned}
\mathbb{E}_{\mathbf{Y}}\left\{P_{\mathrm{c}}\right\} &=\int_{\mathbf{y}} P_{\mathrm{c}} f_{\mathbf{Y}}(\mathbf{y}) d \mathbf{y} \\
&=\frac{1}{C} \sum_{k=1}^{C} \int_{\mathbf{y}} \frac{\Gamma\left(\mathbf{y} \mid H_{k}\right)}{\sum_{k^{\prime}=1}^{C} \Gamma\left(\mathbf{y} \mid H_{k^{\prime}}\right)} \Gamma\left(\mathbf{y} \mid H_{k}\right) d \mathbf{y} \label{equation14}
\end{aligned}
\end{equation}
假设参数集$\mathbf{u_k}$是已知的，但实际上，通常会对其进行建模或估计，但会有一些偏差。然后将(\ref{equation14})表示为:
\begin{equation}
\mathbb{E}_{\mathbf{Y}}\left\{P_{\mathrm{c}}\right\}=\mathbb{E}_{\mathbf{U}_{k}, \hat{\mathbf{U}}_{k}, \mathbf{Y}}\left\{\frac{1}{C} \sum_{k=1}^{C} \int_{\mathbf{y}} \frac{\Gamma\left(\mathbf{y} \mid H_{k}, \hat{\mathbf{u}}_{k}\right)}{\sum_{k^{\prime}=1}^{C} \Gamma\left(\mathbf{y} \mid H_{k^{\prime}}, \hat{\mathbf{u}}_{k^{\prime}}\right)}\right\}\label{equation15}
\end{equation}
其中$\widehat{\mathbf{u_k}}$是$\mathbf{u_k}$的估计值。很难得到(\ref{equation14})和(\ref{equation15})的闭合解，但可以用蒙特卡罗方法得到数值近似。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{信号段}\label{sec:SignalSeg}

将归一化似然向量定义为$\xi^N=[\xi^N_1,\dots,\xi^N_C]^T$，其中对应的调制元素为$\xi^N_k=P_{cc}(\mathbf{y}|H_k)$，显然
$\sum_{k=1}^C \xi^N_k = 1$。
因为观测序列中的时刻是独立且相同的分布(\textbf{I.I.D})，所以整个向量可以分割成$S$个连续的子序列，对应的长度为$N_S$，$N=\sum^S_{s=1}N_s$。此外，结合(\ref{equation9})和(\ref{equation12})，$\xi^N_K$可以写为:
\begin{equation}
\begin{aligned}
\xi_{k}^{N} &=\frac{\prod_{s=1}^{S} \Gamma\left(\mathbf{y}^{N_{s}} \mid H_{k}\right)}{\sum_{k^{\prime}=1}^{C} \prod_{s=1}^{S} \Gamma\left(\mathbf{y}^{N_{s}} \mid H_{k^{\prime}}\right)} \\
&=\frac{\prod_{s=1}^{S} \xi_{k}^{N_{s}}}{\sum_{k^{\prime}=1}^{C} \prod_{s=1}^{S} \xi_{k^{\prime}}^{N_{s}}} \\
&=\gamma_{k}\left(\boldsymbol{\xi}^{N_{1}}, \ldots, \boldsymbol{\xi}^{N_{S}}\right)\label{equation16}
\end{aligned}
\end{equation}
其中$\xi^{N_s}$表示分段$s$的归一化似然向量，当$N$较大时有意义，我们将其分成几个分段进行分块计算。更重要的是，(\ref{equation16})表示分段等同于原来的分类任务，分块计算的设计可以灵活可实现。

\section{基于CNN的分类器}\label{CNNClassifier}

我们提出了一种新的端到端DNN使能AMC~\cite{r32}-~\cite{r34}，它与ML-AMC非常接近，并显著提高了推理速度。
\subsection{在AMC中使用CNN的原因}\label{useCNNreason}

假设信道环境是时间不变和频率非选择性的，则观测序列$y$中的元素是独立且相同的分布(\textbf{I.I.D})，$y$的一些特性必须注意：
\begin{itemize}
\item 任意时间间隔内的分段$y^{N_s}$的统计特征是相同的，即:
\begin{equation}
\begin{aligned}
\mathbb{E}\left\{\mathrm{L}\left(\mathbf{y}_{i}^{N_{s}} \mid H_{k}, \mathbf{u}_{k}\right)\right\} &=\mathbb{E}\left\{\mathrm{L}\left(\mathbf{y}_{j}^{N_{s}} \mid H_{k}, \mathbf{u}_{k}\right)\right\}, \\
0 & \leq i, j<N-N_{s},\notag
\end{aligned}
\end{equation}
其中，$\boldsymbol{y}_i^{N_s}=[y_i,\dots,y_{i+N_s−1}]^T$
\item 信号序列$y_n$可以被分成$S$个连续的子序列段，并且似然函数可以表示为$\boldsymbol{L}(\boldsymbol{y_n}|H_k)=\sum^{N_S}_{s=1}\boldsymbol{L}(\boldsymbol{y^{N_s}}|H_k)$。
\item 任意交换$y$中的元素任意次，得到$\boldsymbol{y}$的似然函数为$\boldsymbol{L}(\boldsymbol{y}'|H_k)=\boldsymbol{L}(\boldsymbol{y}|H_k)$。
\end{itemize}
% \begin{figure}
%   	\begin{overpic}[width=\columnwidth]{histogram.pdf}
%     \end{overpic}
%     \caption{给定一个输入图像(左)，我们计算其颜色直方图(中)。
%         直方图中每一个bin所对应的颜色在下方的横条中显示。
%         量化后的图像仅仅使用$43$个直方图bin的色彩，
%         且仍然保留了显著性检测所需的足够的视觉质量。
%     }\label{fig:colorFre}
% \end{figure}

性质1表示信号在时域中均匀分布。具有卷积核的信号段在不同时刻的冲激响应是相同的，因此可以在任何时隙提取或检测特征。性质2证明似然函数是合成的，并且可以由堆叠的级联表示模型来近似。因此，具有局部感受场的CNN比全连接的NN更适用。此外，在给定较大的N的情况下，对于全连接模型，训练过程中的过拟合问题变得更加严重。
性质3表示信号是作为时间序列进行采样的，但它与时间无关，因此不建议使用递归神经网络(RNN)来解决此问题。
\subsection{网络架构}\label{networkarchitecture}

我们设计的CNN-AMC的结构如\figref{fig:chen1}所示，输入由两部分组成：维数$N=1000$的预处理观测序列$y$和与ML-AMC相同的估计符号信噪比。然而，$y$是一个原始信号向量，需要进行卷积才能生成更高级别的信息，而SNR只是一个标量，不能卷积。不同类型的信号不能简单地串联起来作为CNN-AMC的输入。取而代之的是使用具有多个输入的混合神经网络。

为了将复矢量$y$变换成实数数据，将实部和虚部分开，然后将其转换成形状$2\times N$。例如，对于第一卷积层$l=1$，输入矢量可以表示为$\boldsymbol{X}^{l−1}=\boldsymbol{y}\in \mathbb{R}^{N_{l−1}\times V^{l−1}}$，其中矢量数量和维数分别为$N_{l−1}=2$和$V_{l−1}=1000$。输出向量的大小列在\figref{fig:chen1}中相应层框的左侧。对于具有$N_{l−1}$输入和$N_l$输出向量的一维卷积层$l$，存在$N_l\times N_{l−1}$卷积核(也称为权重矩阵)和$N_l$偏置，核大小用$k_l$表示。给出上一层的输出向量$\mathbf{x}^{l−1}_i,
i \in \mathcal{N}_{l-1}=\left\{1, \ldots, N_{l-1}\right\}
$当前层的第i个输出向量表示为:
\begin{equation}
\mathbf{x}_{i}^{l}=f\left(\sum_{j=1}^{N_{l-1}} \mathbf{x}_{j}^{l-1} * \mathbf{k}_{i, j}^{l}+b_{i}^{l}\right)\label{equation17}
\end{equation}
其中，采用整流线性单元(RELU)~\cite{r35}作为快速非线性激活函数：$f(x)=max(0,x)$，并且$∗$是卷积操作，在卷积层l中有$N_l\times N_{l−1}\times K_l+N-l$可训练参数。合并层也称为子采样层，它产生输入向量的下采样版本。形式上，我们将第$i$个输出向量设为：
\begin{equation}
\mathbf{x}_{i}^{l+1}=\operatorname{down}\left(\mathbf{x}_{i}^{l}\right)\label{equation18}
\end{equation}
 \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen1-2868698-large.pdf}
  \end{overpic}
     \caption{以矢量和标量作为输入的基于CNN的调制分类器的图示。文本‘Conv 12×3，1relu’表示在该卷积层中，核数、核大小和步长分别为12、3和1。类似地，文本‘AveragePool 2，1’表示在该平均池化层中，窗口大小和步幅分别为2和1。
     }\label{fig:chen1}
 \end{figure}
其中，$down(·)$表示子采样函数，并且平均汇集~\cite{r36}用于通过计算加窗输入向量的平均值来降低维数。在堆叠卷积层和池化层之后，输出向量被展平的层压缩成向量。然后使用致密层将该向量编码成包含提取的高层信息的256维向量。对于具有输入矢量$X^{l−1}$的致密层l，输出矢量可以描述为:
\begin{equation}
\mathbf{x}^{l}=f\left(\mathbf{W}^{l} \mathbf{x}^{l-1}+\mathbf{b}^{l}\right)\label{equation19}
\end{equation}
其中$\boldsymbol{W^l}\in \mathbb{R}^{N_1×N_{l−1}}$和$\boldsymbol{b}^l\in \mathbb{R}^{N_1}$分别是权重矩阵和偏置。这是一个全连接层，可训练参数的数量为$N_l\times N_{l−1}+N_l$。然后将该向量与标量信噪比的矢量化表示级联。
在另一致密层之后，然后，由激活函数为Softmax的输出层产生归一化似然向量，即$\widehat{\xi}^N$。输出$\widehat{\xi}^N = [\widehat{\xi}^N_1,\dots,\widehat{\xi}^N_C]^T$，元素$\widehat{\xi}^N_i$定义为：
\begin{equation}
\hat{\xi}_{i}^{N}=\frac{e^{x_{i}}}{\sum_{j=1}^{C} e^{x_{j}}}\label{equation20}
\end{equation}
其中$x_j$是第$j$个预激活输出。最后，CNN-AMC的表示可以描述如下：
\begin{equation}
\hat{\boldsymbol{\xi}}^{N}=F(\mathbf{y}, \mathrm{SNR} ; \theta)\label{equation21}
\end{equation}
其中$\theta$是网络参数，$F(·)$是基于CNN的分类器的总体函数。在该深层模型中，特征提取由CNN部分自动实现，高层主要处理输入输出数据之间的映射关系。通常，CNN-AMC是目标函数(9)的强大非线性逼近器$F(·)$。完整的训练/测试和仿真源代码可以从\url{https://github.com/mengxiaomao/cnn_amc}下载。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{两步式训练}\label{Two-step}
在一般的训练过程中，CNN-AMC是逐时段训练的，并且在一个时段内，所有训练样本被用来训练CNN-AMC一次。在一个历元内，对整个训练集进行洗牌，分成大小为$N_b$的批次，逐批次地对模型进行训练。本文采用分类交叉熵误差作为损失函数，将训练批次的损失函数表示为:
\begin{equation}
L(\theta)=-\frac{1}{N_{\mathrm{b}}} \sum_{i=1}^{N_{\mathrm{b}}} \boldsymbol{\xi}_{i}^{N} \log \hat{\boldsymbol{\xi}}_{i}^{N}+\left(1-\boldsymbol{\xi}_{i}^{N}\right) \log \left(1-\hat{\boldsymbol{\xi}}_{i}^{N}\right)\label{equation22}
\end{equation}
其中$\boldsymbol{\xi}_{i}^{N}$表示目标输出。该损失函数通过随机梯度下降(SGD)算法最小化，并且模型参数被迭代地更新为:
\begin{equation}
\theta_{t+1}=\theta_{t}-\eta \nabla L\left(\theta_{t}\right)\label{equation23}
\end{equation}
其中$\eta$表示学习率，操作$\nabla$表示梯度运算符。作为SGD算法的一种变体，本文采用了自适应矩估计(ADAM)~\cite{r37}算法，并采用了缺省设置。ADAM优化器通过对低阶动量的自适应估计，动态地自适应调整$\eta$以加快收敛速度，并被证明是有效的，特别是对于深部模式。记录每个训练时段的参数集$\theta$和验证损失，并定义最优的验证损失最小的CNN-AMC。仿真中可以产生不同信噪比下不同长度的接收观测序列，ML-AMC还可以产生大量的输出数据。

然而，在我们的实验中，由于模型复杂，目标函数复杂，基于CNN的分类器的直接训练具有挑战性。通过大量的训练数据和足够的迭代次数，我们发现损失函数$L(\theta)$不能下降，说明该模型从一开始就停留在高维参数优化上。
虽然最后一项任务很困难，但对于模型来说，学习更简单的函数会容易得多。有一个使用辅助数据训练基于CNN的分类器的简单但有用的技巧。如\figref{fig:chen2}所示，整个培训过程分为两个步骤。
 \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen2-2868698-large.pdf}
  \end{overpic}
     \caption{CNN-AMC的两步训练说明。在步骤1中，所有初级输出元素用橙色圆圈表示，而附加噪声项用灰色圆圈表示。在步骤2中去除高斯噪声的输出元素。步骤1中的输入和输出用$x_I$和$y_I$表示，类似地，步骤2用$x_{II}$和$y_{II}$表示。
     }\label{fig:chen2}
 \end{figure}

\begin{table}[]
\caption{两步训练的仿真参数}
\begin{tabular}{c|c|c}
\hline 训练步骤 & 步骤 1 & 步骤 2 \\
\hline Epoch 数量 & 20 & 240 \\
SNR 范围/dB & {$[0,10]$} & {$[-6,12]$} \\
训练数据量 & 79,200 & 228,060 \\
验证数据量 & 8,800 & 25,340 \\
使用ML分类器的输出 & No & Yes \\
\hline
\end{tabular}
\label{tab:two-step}
\end{table}
\begin{itemize}
\item \textit{预训练}：此步骤旨在帮助CNN-AMC在训练开始时收敛，因此训练数据的规模很小，如表I所示。本步骤中增加了另一个带灰色圆圈的输出项，其对应的类别是零均值的高斯白噪声，而不是其他调制方案。噪声的辅助训练样本量与任何其他调制方案相同，其$I/O$数据用$\mathbf{x}_{aux}$和$\mathbf{y}_{aux}$表示。输出向量被设置为二进制。在仿真测试中，损失函数$L(\theta)$减小，梯度方差变小，训练一段时间后收敛。存储验证损失最小的参数$\theta$，以便在下一次训练中进行模型初始化。
\item \textit{微调}：首先，第一步，CNN-AMC加载存储的参数$\theta$，辅助输出项在最终分类中是冗余的，所以用主输出层替换顶层，然后随机初始化。在该步骤中，仅生成调制方案的样本，并且ML-AMC以归一化似然向量$\xi^N$的形式产生训练数据的输出，这与第一步不同。精心设计的训练数据帮助CNN-AMC逼近ML-AMC，仿真结果表明，使用ML-AMC生成的数据比使用普通二进制标记的$P_c$性能更接近ML-AMC。类似地，验证损失最低的CNN-AMC在以下模拟中被存储、重新加载和测试，详细设置如表\ref{tab:two-step}所示。
\end{itemize}
\begin{table*}[htbp]
\centering
\caption{两步训练综述}
\begin{tabular}{l}
\hline \hline 步骤 1: \\
1) 生成训练数据，并且包括附加高斯噪声的样本。 \\
2) 随机初始化CNN-AMC参数。 \\
3) 训练CNN-AMC，以最小的验证损失存储模型参数。\\
\hline 步骤 2: \\
1) 产生训练数据，并从ML-AMC获得目标输出。 \\
2) 更换CNN-AMC的顶层并随机初始化；从存储器加载其他模型参数。 \\
3) 训练CNN-AMC，以最小的验证损失存储模型参数。 \\
\hline \hline
\end{tabular}\label{tab:2}
\end{table*}
 \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen3-2868698-large.pdf}
  \end{overpic}
     \caption{准确性和损失的培训/验证曲线。X轴表示训练时期。验证和训练曲线用深蓝色和橙色线条绘制，精确度和损耗曲线用实线和虚线表示。
     }\label{fig:chen3}
 \end{figure}
 
 表\ref{tab:2}给出了两步训练方法的总结。在\figref{fig:chen3}中，绘制了完整的两步训练过程中的训练/验证损失/精度曲线。一般来说，损耗曲线在两个阶段都是下降的，而精度曲线是上升的。在第二步中，训练损失在开始时急剧下降，然后稳步下降。验证损失曲线与训练损失曲线具有相似的趋势。随着历时的增加，训练时间趋于平稳，抖动较小，很难低于0.47，即训练处于饱和区。存储具有最小验证损失的相应θ，并用红点标记步骤2中的最小值。

复平面上的辅助高斯噪声的似然函数是拟凸的，这是一种比较直观的逼近函数。从(\ref{equation7})中我们知道，一个观测时刻的条件概率密度函数是具有不同均值和相同方差的高斯分布的加权和。因此，对于已经学习了高斯噪声函数的CNN-AMC来说，对于其他调制方案的更复杂的PDF的近似变得舒适得多。在学习了符号的近似值之后，根据(\ref{equation9})，目标函数变得更容易获得。在第一步中，解决了初始收敛问题。通过预训练，CNN-AMC在第二步中被进一步训练为尽可能接近最优ML-AMC。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{迁移学习}\label{Transfer Learning}
上述两步训练处理的问题是：给定具有随机参数初始化的CNN-AMC，以及如何训练它以利用给定的训练和验证数据来分类调制方案。事实上，一个人学到的CNN-AMC可以应用于一个特定的任务，例如具有几种候选调制模式的相干环境。一旦场景或调制集改变，CNN-AMC需要用新的训练数据重新训练。

事实上，没有必要在迁移学习的帮助下从一开始就重新启动新的培训过程~\cite{r38}。
如前所述，CNN-AMC的CNN部分从经过预处理的原始观测序列中产生更高级别的信息。特征提取可以在不同的任务中普遍使用，并且可以利用一些已经学习到的先验知识或经验而不是纯粹的空白来更容易地进行训练。这里，我们以两个案例为例：
\begin{itemize}
\item \textit{多种调制方式}：在不同的情况下，根据分类任务的要求，增加或删除一些调制方式。除了新的训练数据外，CNN-AMC的输出层需要更改为新的相应类别。
因此，除了最上层的参数外，所有CNN-AMC参数都可以作为初始化加载，而不是随机设置。
\item \textit{不同的环境}：在预处理期间，载波相位补偿包括在相干情况下，但不包括非相干情况。使用提出的两步法可以用新的非相干训练数据来训练CNN-AMC。此外，在一致性情况下学习的基于CNN的分类器可以作为初始模型使用这些数据进行训练，所有的模型参数都是从一致性情况下训练的模型参数中加载的。我们给出了一个使用迁移学习的例子，与之对应的是使用两步训练的例子。如图\ref{fig:chen4}所示，损失曲线的下降速度比使用两步训练的速度快。此外，可以省去训练前的步骤，进一步降低了训练周期的时间成本。
\end{itemize}

虽然迁移学习有助于提高训练效率，但它是通过两步学习来发现的。
 \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen4-2868698-large.pdf}
  \end{overpic}
     \caption{验证数据的丢失和准确度随历元时间的变化曲线。与两步训练算法相比，采用转移学习的算法达到饱和区域所需的时间更短，并且减少了预训练步骤。
     }\label{fig:chen4}
 \end{figure}
  \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen5-2868698-large.pdf}
  \end{overpic}
     \caption{7种调制方式的$P_{cc}(\mathbf{y}|H_k)$曲线，信噪比范围为[−6，10]dB。ML分类器和基于CNN的分类器分别用实线和虚线描述。
     }\label{fig:chen5}
 \end{figure}
\subsection{单位分类器}\label{Unit Classifier}

为了对低信噪比的信号实现高分类精度，序列空间N可以高达几个甚至数万。随着N的增加，需要更多的训练样本来克服过拟合问题，从而使训练变得更加困难。问题是在有限的存储空间和计算资源下，分类及其对应的$N$的训练可能很难，甚至不切实际。另一方面，设计一种输入空间可变的分类器也是一项具有挑战性的工作。

根据前面在第\ref{Two-step}小节中的分析，大$N$的输入$y$可以被分成具有单位长度的段。将输入信号$y$为单位输入维数的基于CNN的分类器定义为单位分类器。从理论上讲，在相干环境下，该分段与完全观测分段具有相同的分类性能。更重要的是，单元分类器关于训练、缓存和计算的比例是可调整的。因此，AMC的设计可以灵活地适应硬件，与序列空间$N$无关，也可以对所有的单元分类器并行进行推理。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{仿真结果}\label{simulation}

通过研究，AMC的通用序列空间N从数百到数千不等，并考虑到硬件条件，对观测空间$N=500$(命名为CNN-500)和$N=1000$(命名为CNN1000)的CNN-AMC进行了测试。此外，将ML-AMC与基于9个高阶累积量的基于特征的AMC~\cite{r39}(称为Cu-AMC)进行了比较。采用二阶、四阶和六阶累积量作为特征，采用两层隐层神经网络作为分类器。考虑了几种调制方案。总体正确概率和条件正确概率对信噪比的影响是需要研究的两个主要问题。此外，还必须考虑对估计误差的鲁棒性、推理时间开销和系统存储需求等多个方面。
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{细节}\label{coherent}

仿真从相干环境开始，考虑了先验概率相等的7种调制方式，即2PSK、4PSK、8PSK、16QAM、16APSK、32APSK和64QAM。假设接收器具有精确的参数集$\mathbf{u_k}$的知识，包括恒定相位偏移$\theta_c$和SNR，但是需要推断调制方案。除了(\ref{equation6})的功率归一化之外，在预处理周期中用共轭相位偏移来补偿观测序列。
\begin{equation}
\mathbf{y}=\mathbf{y} e^{-j \theta_{c}}\label{equation24}
\end{equation}
结合(\ref{equation7})、(\ref{equation8})和(\ref{equation9})，假设$H_k$下的似然函数由下式显式给出：
\begin{equation}
\Gamma\left(\mathbf{y} \mid H_{k}\right) \propto \frac{1}{M_{k}} \prod_{n=0}^{N-1} \sum_{i=1}^{M_{k}} \exp \left(\frac{\left|x_{n}^{k, i}\right|^{2}-2 \Re \mathfrak{e}\left\{y_{n}^{*} x_{n}^{k, i}\right\}}{N_{0}}\right)
\end{equation}
因此，可以得到归一化似然向量$\xi^N$，并将其与观测序列$y$相结合，可以为CNN-AMC生成大量的训练和验证数据。

\begin{itemize}
\item \textit{正确概率}：支持深度学习的AMC比ML-AMC快得多。然而，作为最优AMC的近似值，它在本仿真研究的$P_{CC}(\mathbf{y}|H_K)$和$P_c$方面可能会受到一定程度的性能下降。

首先，7种调制的条件正确概率$P_{CC}(\mathbf{y}|H_K)$曲线如\figref{fig:chen5}($N=1000$)所示。
ML-AMC用实线表示，CNN-1000用虚线绘制。显然，2PSK是我们集合中最容易识别的调制，即使在低信噪比的情况下，它的条件正确概率也是100\%。与其他调制方式相比，4PSK和8PSK更容易识别，当$SNR=2dB$和$SNR=4dB$时，其$P_{CC}(\mathbf{y}|H_K)$分别达到1.00。在高信噪比($SNR=10dB$)下，ML-AMC几乎可以毫无错误地识别出所有的调制方式。

通常，将CNN-1000与ML-AMC进行比较时，$P_{CC}(\mathbf{y}|H_K)$性能略有下降。在某些SNR区域，当使用CNN-1000时，对于某些调制，$P_{CC}(\mathbf{y}|H_K)$高于ML-AMC。例如，在信噪比从−6到−2dB并且调制方案是16QAM的情况下，CNN-1000似乎比ML-AMC更好。这是因为相应的条件概率被高估，代价是某些其他调制方案(例如64QAM)的较低的正确分类概率。
CNN-1000的$P_c$只能无限逼近ML-AMC，不能超过最优值。


\item \textit{不同的环境}：在预处理期间，载波相位补偿包括在相干情况下，但不包括非相干情况。使用提出的两步法可以用新的非相干训练数据来训练CNN-AMC。此外，在一致性情况下学习的基于CNN的分类器可以作为初始模型使用这些数据进行训练，所有的模型参数都是从一致性情况下训练的模型参数中加载的。我们给出了一个使用迁移学习的例子，与之对应的是使用两步训练的例子。如图\ref{fig:chen4}所示，损失曲线的下降速度比使用两步训练的速度快。此外，可以省去训练前的步骤，进一步降低了训练周期的时间成本。
\end{itemize}
  \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen6-2868698-large.pdf}
  \end{overpic}
     \caption{不同序列空间N对信噪比的$P_c$曲线。ML-AMC、CNN-500和CNN-1000分别用实线、虚线和虚线表示。
     }\label{fig:chen6}
 \end{figure}
  \begin{figure}[t!]
   \begin{overpic}[width=\columnwidth]{chen7-2868698-large.pdf}
  \end{overpic}
     \caption{集合$\{3^{\circ},6^{\circ},9^{\circ},12^{\circ},15^{\circ}    \}$中不同相移$|\nabla \theta_c|$估计误差的$P_c$曲线。ML和CNN-1000分类器分别用实线和虚线表示。
     }\label{fig:chen7}
 \end{figure}
\paragraph{致谢.} 本项目受到了国家973计划(2011CB302205)， 国家863计划(2009AA01Z327)，
国家自然科学基金(U0735001)，以及国家核高基计划(2011ZX01042-001-002)的支持.
程明明的工作受到了Google PhD fellowship, IBM PhD fellowship, 以及教育部博士研究生学术新人奖的资助。

{\small
\bibliographystyle{ieeetr} 
\bibliography{Saliency}

}

% \end{CJK*}
\end{document}
